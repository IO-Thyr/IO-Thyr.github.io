[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "The Quest for understanding Distribution Distance: From Theory to Practice\n\n\n\nInformation Theory\n\n\nMath\n\n\n\n\n\n\n\nTancrede Hayer\n\n\nDec 27, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bienvenue sur mon Site !",
    "section": "",
    "text": "Vous trouverez i√ßi un portail d‚Äôacc√®s sur qui je suis et ce que je fais.\nThis is a gateway to who I am and what I do.\n\n\n\n\n\n\n\n\n\n\nIl n‚Äôy a rien de bon ni de mauvais sauf ces deux choses : la sagesse qui est un bien, l‚Äôignorance qui est un mal.\nThere is nothing good or bad except these two things: wisdom, which is good, and ignorance, which is evil.\nEuthyd√®me, 281e de Platon\n\n\n\n\nüí° Data Scientist qui aime apprendre\nDe l‚Äôintelligence artificielle aux sciences ‚Äúclassique‚Äù, je me consacre √† comprendre et √† partager tout ce qui peux m‚Äôint√©resser.\nüìä Cr√©ateur de contenu\nLorsque je ne calcule pas de chiffres ou ne d√©veloppe pas d‚Äôalgorithmes, vous me trouverez le soir en train de cr√©er du contenu sur ma cha√Æne YouTube - un t√©moignage de mon c√¥t√© cr√©atif, o√π art et technologie se rencontrent !\nüéØ Mission\nRendre les concepts complexes accessibles, stimuler la curiosit√© et encourager l‚Äôexploration de la science et de la technologie.\n\nüí° A Data Scientist with a passion for learning\nFrom artificial intelligence to ‚Äúclassical‚Äù sciences, I dedicate myself to understanding and sharing everything that piques my curiosity.\nüìä Content Creator\nWhen I‚Äôm not crunching numbers or developing algorithms, you‚Äôll find me in the evening creating content on my YouTube channel ‚Äì a testament to my creative side, where art and technology meet!\nüéØ Mission\nTo make complex concepts accessible, spark curiosity, and encourage the exploration of science and technology."
  },
  {
    "objectID": "posts/2024-27-12-distribution-divergence/index.html",
    "href": "posts/2024-27-12-distribution-divergence/index.html",
    "title": "The Quest for understanding Distribution Distance: From Theory to Practice",
    "section": "",
    "text": "Have you ever wondered how we measure how ‚Äúfar apart‚Äù two distributions are in data science? It‚Äôs not as simple as it might seem, but don‚Äôt worry ‚Äì we‚Äôll keep the math tame and focus on intuition.\nBy the end of this reading, you will have a better understanding of optimal transport theory and how it helps us in data science and model monitoring."
  },
  {
    "objectID": "posts/2024-27-12-distribution-divergence/index.html#the-kullback-leibler-divergence",
    "href": "posts/2024-27-12-distribution-divergence/index.html#the-kullback-leibler-divergence",
    "title": "The Quest for understanding Distribution Distance: From Theory to Practice",
    "section": "2.1 The Kullback-Leibler divergence",
    "text": "2.1 The Kullback-Leibler divergence\nImagine you‚Äôre in a bustling city, and you have two maps, \\(P\\) and \\(Q\\), trying to guide you through the streets. Now, we want to know which map is more reliable when navigating from point A to B (by comparing an outdated map vs a new official map). Here comes KL divergence, to help us out!\nThe KL Divergence measures how one probability distribution \\(Q\\) diverges from another, \\(P\\) :\n\\[\nD_{KL}(P \\| Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}\n\\]\nFor continuous distributions it is :\n\\[\nD_{KL}(P \\| Q) = \\int_{-\\infty}^{\\infty} p(x) \\log \\frac{p(x)}{q(x)} dx\n\\]\nIn this cityscape analogy, the KL divergence measures how much extra walking you‚Äôd do if you used map \\(Q\\) instead of the true map \\(P\\). When following directions from \\(Q\\), the additional distance traveled is proportional to \\(D_{KL}(P \\| Q)\\). It‚Äôs like a penalty for when \\(Q\\) underestimates or misrepresents the probability of going through certain streets compared to \\(P\\).\n\n\n\n\n\n\nConcrete cityscape exemple\n\n\n\n\n\nLet‚Äôs make this concrete with a simple city layout having three key locations: Home (H), Work (W), and Shopping Mall (S).\nThe true map \\(P\\) shows these probabilities of successful routes:\n\n\\(P(H‚ÜíW) = 0.5\\) (a direct highway route)\n\\(P(H‚ÜíS) = 0.3\\) (a clear urban route)\n\\(P(H‚ÜíM) = 0.2\\) (a residential path)\n\nWhile the approximate map \\(Q\\) shows:\n\n\\(Q(H‚ÜíW) = 0.4\\) (underestimates highway access)\n\\(Q(H‚ÜíS) = 0.4\\) (overestimates this route)\n\\(Q(H‚ÜíM) = 0.2\\) (correctly estimates this one)\n\nLet‚Äôs calculate this for our city example:\n\nFor the Work route: \\[P(H‚ÜíW) * log(P(H‚ÜíW)/Q(H‚ÜíW)) = 0.5 * log(0.5/0.4) ‚âà 0.048\\]\nFor the Shopping route: \\[P(H‚ÜíS) * log(P(H‚ÜíS)/Q(H‚ÜíS)) = 0.3 * log(0.3/0.4) ‚âà -0.037\\]\nFor the Mall route: \\[P(H‚ÜíM) * log(P(H‚ÜíM)/Q(H‚ÜíM)) = 0.2 * log(0.2/0.2) ‚âà 0\\]\n\nTotal KL divergence = 0.048 -0.037 + 0 = 0.011\nThis means using map Q instead of P results in about 1.1% inefficiency in navigation. In other words, you‚Äôll spend about 1.1% more time or distance traveling when using the approximate map Q compared to the true map P.\n\n\n\nThe KL divergence quantifies the inefficiency of assuming that the distribution \\(Q\\) approximates the true distribution \\(P\\). When encoding data points sampled from \\(P\\) using a code optimized for \\(Q\\), the additional encoding length is proportional to \\(D_{KL}(P \\| Q)\\).\nNow consider, for example, the log term \\(\\log \\frac{P(x)}{Q(x)}\\). If \\(P\\) suggests you go down an alley while \\(Q\\) shows it‚Äôs a dead end (i.e., \\(Q(x)\\) approaches zero but \\(P(x)\\) doesn‚Äôt), that‚Äôs when our navigator (KL divergence) really starts warning you, ‚ÄúWatch out! You‚Äôre about to take a long detour!‚Äù\nThe log term, \\(\\log \\frac{P(x)}{Q(x)}\\), doesn‚Äôt just warn you - it declares the entire route using Q as invalid (by giving it an infinite penalty) for underestimating probabilities in \\(Q\\) compared to \\(P\\).\nKL divergence heavily penalizes cases where \\(Q(x)\\) approaches zero while \\(P(x)\\) remains non-zero.\nBefore continuing let‚Äôs keep in mind the special properties of KL divergence through this lens:\n\nNon-Negativity: \\(D_{KL}(P \\| Q) \\geq 0\\) - Using either map will never lead you astray; at worst, they‚Äôll guide you equally well (when \\(P = Q\\)).\nZero Minimum: \\(D_{KL}(P \\| Q) = 0 \\iff P = Q\\) - The only way our navigator won‚Äôt make you walk extra is if both maps are the same.\nAsymmetric: \\(D_{KL}(P \\| Q) \\neq D_{KL}(Q \\| P)\\) - Our navigator isn‚Äôt biased ; he‚Äôll give different walking distance estimates depending on which map you‚Äôre using as your reference."
  },
  {
    "objectID": "posts/2024-27-12-distribution-divergence/index.html#jensenshannon-divergence",
    "href": "posts/2024-27-12-distribution-divergence/index.html#jensenshannon-divergence",
    "title": "The Quest for understanding Distribution Distance: From Theory to Practice",
    "section": "2.2 Jensen‚ÄìShannon Divergence",
    "text": "2.2 Jensen‚ÄìShannon Divergence\nContinuing our city analogy, imagine you have two different navigation systems (representing distributions P and Q) giving you directions from point A to B. Using KL divergence, you could measure how much extra distance you‚Äôd walk if you followed Q‚Äôs directions when P actually knows the correct route (KL(P||Q)), or vice versa (KL(Q||P)). But what if, instead of just measuring how wrong one system is compared to the other, you want to understand how different these two navigation systems are from each other? Here‚Äôs where the Jensen-Shannon (JS) divergence comes in.\nWe can see the JS Divergence as a symmetrized and smoother version of the KL Divergence, defined as:\n\\[\nD_{JS}(P \\| Q) = \\frac{1}{2} D_{KL}(P \\| M) + \\frac{1}{2} D_{KL}(Q \\| M), \\text{ where } M = \\frac{1}{2}(P + Q)\n\\]\n\n\n\n\n\n\nConcreate example\n\n\n\n\n\nDetailed Calculation of Jensen-Shannon Divergence\nLet‚Äôs work through a detailed calculation using our city navigation example to better understand how JS divergence works in practice.\n\nStep 1: Calculate M\n\nGiven our probability distributions: P = [0.5, 0.3, 0.2] (true map) Q = [0.4, 0.4, 0.2] (approximate map)\nThe average distribution M is calculated as:\n\\[\nM = \\frac{1}{2}(P + Q) = \\left[\\frac{0.5 + 0.4}{2}, \\frac{0.3 + 0.4}{2}, \\frac{0.2 + 0.2}{2}\\right] = [0.45, 0.35, 0.2]\n\\]\n\nStep 2: Calculate D_KL(P||M)\n\nThe KL divergence from M to P is calculated as:\n\\[\nD_{KL}(P \\parallel M) = \\sum_{i} P(i) \\cdot \\log\\left(\\frac{P(i)}{M(i)}\\right)\n\\]\nLet‚Äôs calculate each term:\n\nFor H‚ÜíW route: \\[\nP(0.5) \\cdot \\log\\left(\\frac{0.5}{0.45}\\right) = 0.5 \\cdot \\log(1.1111) = 0.5 \\cdot 0.0458 = 0.0229\n\\]\nFor H‚ÜíS route: \\[\nP(0.3) \\cdot \\log\\left(\\frac{0.3}{0.35}\\right) = 0.3 \\cdot \\log(0.8571) = 0.3 \\cdot (-0.0669) = -0.0201\n\\]\nFor H‚ÜíM route: \\[\nP(0.2) \\cdot \\log\\left(\\frac{0.2}{0.2}\\right) = 0.2 \\cdot \\log(1) = 0\n\\]\nSum \\[\nD_{KL}(P \\parallel M) = 0.0229 - 0.0201 + 0 = 0.0028\n\\]\n\n\nStep 3: Calculate D_KL(Q||M)\n\nSimilarly for Q:\n\\[\nD_{KL}(Q \\parallel M) = \\sum_{i} Q(i) \\cdot \\log\\left(\\frac{Q(i)}{M(i)}\\right)\n\\]\n\nFor H‚ÜíW route: \\[\nQ(0.4) \\cdot \\log\\left(\\frac{0.4}{0.45}\\right) = 0.4 \\cdot \\log(0.8889) = 0.4 \\cdot (-0.0493) = -0.0197\n\\]\nFor H‚ÜíS route: \\[\nQ(0.4) \\cdot \\log\\left(\\frac{0.4}{0.35}\\right) = 0.4 \\cdot \\log(1.1429) = 0.4 \\cdot 0.0571 = 0.0228\n\\]\nFor H‚ÜíM route: \\[\nQ(0.2) \\cdot \\log\\left(\\frac{0.2}{0.2}\\right) = 0.2 \\cdot \\log(1) = 0\n\\]\nSum: \\[\nD_{KL}(Q \\parallel M) = -0.0197 + 0.0228 + 0 = 0.0031\n\\]\n\n\nStep 4: Calculate D_JS(P||Q)\n\nThe final JS divergence is calculated as:\n\\[\nD_{JS}(P \\parallel Q) = \\frac{1}{2} D_{KL}(P \\parallel M) + \\frac{1}{2} D_{KL}(Q \\parallel M)\n\\]\nSubstituting our values:\n\\[\nD_{JS}(P \\parallel Q) = \\frac{1}{2}(0.0028) + \\frac{1}{2}(0.0031) = 0.0014 + 0.00155 = 0.00295\n\\]\nThe small JS divergence value (0.00295) indicates that our two maps P and Q are fairly similar in their routing suggestions. This makes sense because:\n\nThey completely agree on the probability of the residential path (H‚ÜíM: 0.2)\nThey only slightly disagree on the highway route (0.5 vs 0.4)\nTheir difference in the urban route (0.3 vs 0.4) is moderate\n\nThis numerical example shows how JS divergence provides a symmetric and bounded measure of the difference between two probability distributions, making it particularly useful for comparing different routing strategies or maps.\n\n\n\nThe JS divergence finds a middle ground between maps \\(P\\) and \\(Q\\) by averaging them into a new map \\(M = \\frac{1}{2}(P + Q)\\). It then measures how different this average route is from each original one, giving equal weight to both.\nBy combining and averaging the maps, our navigator (JS divergence) ensures a more stable and symmetric measure of their differences. This interpolation also helps when one map suggests going through a street that the other doesn‚Äôt ‚Äì instead of getting stuck, our navigator finds a sensible alternative route.\nUnlike KL divergence, the JS divergence measures the similarity of two distributions by incorporating their average, \\(M\\). By interpolating between \\(P\\) and \\(Q\\), JS divergence ensures a bounded and symmetric measure. This interpolation resolves issues of infinite divergence where \\(P(x) &gt; 0\\) but \\(Q(x) = 0\\).\nAnd let‚Äôs revisit the special properties of JS divergence using this city analogy:\n\nSymmetric: \\(D_{JS}(P \\| Q) = D_{JS}(Q \\| P)\\) - Our compromising navigator gives equal importance to both maps and provides a balanced measure of their differences, ensuring you‚Äôre not favoring one over the other.\nBounded: \\(0 \\leq D_{JS}(P \\| Q) \\leq 1\\) - The navigator‚Äôs score will always fall within a reasonable range, making it easier for you to gauge how similar or different the two maps are in guiding your journey.\nSmooth: Handles zero probabilities gracefully compared to KL Divergence - When one map suggests going down a street that the other doesn‚Äôt (or vice versa), our navigator won‚Äôt get flustered or lost. She‚Äôll find an alternative route, making sure you still reach your destination with minimal d"
  },
  {
    "objectID": "posts/2024-27-12-distribution-divergence/index.html#a-picture-is-worth-a-thousand-words",
    "href": "posts/2024-27-12-distribution-divergence/index.html#a-picture-is-worth-a-thousand-words",
    "title": "The Quest for understanding Distribution Distance: From Theory to Practice",
    "section": "2.3 A picture is worth a thousand words",
    "text": "2.3 A picture is worth a thousand words\n\nCode\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.optimize import linprog\n\ndef kl_divergence(p, q):\n    p = np.array(p)\n    q = np.array(q)\n    return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n\ndef js_divergence(p, q):\n    p = np.array(p)\n    q = np.array(q)\n    m = 0.5 * (p + q)\n    kl_mq = kl_divergence(m, q)\n    kl_mp = kl_divergence(m, p)\n    return 0.5 * (kl_mq + kl_mp)\n\n# Define two distributions\nx = np.linspace(-3, 3, 100)\np = np.exp(-x**2) / np.sqrt(2 * np.pi)  # Normal distribution, mean=0, std=1\nq = np.exp(-(x - 1)**2) / np.sqrt(2 * np.pi)  # Normal distribution, mean=1, std=1\n\n# Normalize distributions\np /= np.sum(p)\nq /= np.sum(q)\n\n# Compute divergences and their components\nkl_pq = kl_divergence(p, q)\nkl_qp = kl_divergence(q, p)\njs_pq = js_divergence(p, q)\nm = 0.5 * (p + q)\nkl_mq = kl_divergence(m, q)\nkl_mp = kl_divergence(m, p)\n\n# Visualization\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(12, 6))\n\n# Original distributions P and Q\nplt.plot(x, p, label=\"P: N(0,1)\", color=\"blue\")\nplt.plot(x, q, label=\"Q: N(1,1)\", color=\"red\")\n\n# KL(P || Q) component: P * log(P / Q)\nkl_pq_component = np.where(p != 0, p * np.log(p / q), 0)\nplt.plot(x, kl_pq_component, label=f\"KL(P || Q): {kl_pq:.4f}\", color=\"green\", linestyle=\"--\")\n\n# KL(Q || P) component: Q * log(Q / P)\nkl_qp_component = np.where(q != 0, q * np.log(q / p), 0)\nplt.plot(x, kl_qp_component, label=f\"KL(Q || P): {kl_qp:.4f}\", color=\"purple\", linestyle=\"--\")\n\n# JS(P || Q) components: 0.5 * (P * log(P / M) + Q * log(Q / M))\njs_pq_components = 0.5 * (np.where(p != 0, p * np.log(p / m), 0) +\n                         np.where(q != 0, q * np.log(q / m), 0))\nplt.plot(x, js_pq_components, label=f\"JS(P || Q): {js_pq:.4f}\", color=\"orange\", linestyle=\":\")\n\nplt.title(\"KL and JS Divergence Components Visualization\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Probability Density / Component\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFigure¬†1: KL and JS Divergence Components Visualization\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is divergence components ?\n\n\n\n\n\nThe divergence components represent the point-wise contributions to the overall divergence measures between distributions P and Q.\nThe area under each component curve equals the total divergence measure. Locations where components peak indicate regions where the distributions differ most substantially, providing insight into local distribution differences.\n\n\n\nThe plot visualizes the comparison between Kullback-Leibler (KL) and Jensen-Shannon (JS) divergences for two normal distributions \\(PN(0,1)\\) and \\(QN(1,1)\\).\nThe blue and red curves show the original distributions, while the green and purple dashed lines represent the KL divergence components \\(KL(P||Q)\\) and \\(KL(Q||P)\\) respectively, demonstrating KL‚Äôs asymmetry.\nThe orange dotted line shows the JS divergence component, which is symmetric and combines information from both distributions through their midpoint M.\nThe legend includes numerical values for each divergence measure, with \\(KL(P||Q) ‚â† KL(Q||P)\\), illustrating KL‚Äôs asymmetric nature, while JS maintains a single value representing the symmetric difference between the distributions."
  },
  {
    "objectID": "posts/2024-27-12-distribution-divergence/index.html#the-mathematical-journey",
    "href": "posts/2024-27-12-distribution-divergence/index.html#the-mathematical-journey",
    "title": "The Quest for understanding Distribution Distance: From Theory to Practice",
    "section": "4.1 The Mathematical Journey",
    "text": "4.1 The Mathematical Journey\n\n4.1.1 The Classical Approach: Monge‚Äôs Vision\nGaspard Monge, an 18th-century French mathematician, first formulated this problem of optimal transportation who will later be express in terms of moving earth (hence the nickname ‚ÄúEarth Mover‚Äôs Distance‚Äù or ‚ÄúMonge-Kantorovich problem‚Äù). His formulation is the Wasserstein distance for \\(p=1\\):\n\\[\nW_p(P, Q) = \\left( \\inf_{T} \\int_{\\mathbb{R}^d} \\|x - T(x)\\|^p \\, dP(x) \\right)^{1/p}\n\\]\n\\[W_p(Œº, ŒΩ) = \\left(\\inf_{\\gamma \\in \\Gamma(\\mu, \\nu)} \\int_{X \\times Y} d(x,y)^p d\\gamma(x,y)\\right)^{1/p}\\]\n\nA transport map \\(T(x)\\) moves mass at point \\(x\\) under distribution \\(P\\) to some point under distribution \\(Q\\).\nThe cost of moving mass from \\(x\\) to \\(T(x)\\) is given by \\(\\|x - T(x)\\|^p\\), raised to the power \\(p\\).\nWe seek the transport map \\(T\\) that minimizes this total transport cost, denoted as \\(\\inf_T \\|x - T(x)\\|^p\\).\n\nThink of this as finding the optimal way to move each grain of sand from one pile (\\(P\\)) to another (\\(Q\\)). The function \\(T(x)\\) tells us where each grain should go, and we want to minimize the total effort of moving all the sand.\nBut there was a catch ‚Äì sometimes this direct mapping approach proves impossible (\\(T(x)\\) does not exist). Imagine trying to transform a single mountain (\\(P\\)) into two smaller hills (\\(Q\\)). How do you map one peak to two? This limitation led to a more flexible approach.\n\nCode\n# Set up the initial and target distributions of sand piles\nnp.random.seed(42)\npile_1 = np.random.normal(loc=0, scale=1, size=50)  # Smaller sample for clarity\npile_2 = np.random.normal(loc=2, scale=1.5, size=50)  # Target pile\n\n# Transport plan: simplest one-to-one matching (sorted values for illustration)\npile_1_sorted = np.sort(pile_1)\npile_2_sorted = np.sort(pile_2)\n\n# Create the multiplot\nfig, axes = plt.subplots(1, 3, figsize=(18, 5), sharey=False)\n\n# Plot initial pile\nsns.histplot(pile_1, kde=True, color=\"blue\", ax=axes[0])\naxes[0].set_title(\"Initial Pile of Sand\")\naxes[0].set_xlabel(\"Sand Position\")\naxes[0].set_ylabel(\"Frequency\")\n\n# Plot target pile\nsns.histplot(pile_2, kde=True, color=\"green\", ax=axes[1])\naxes[1].set_title(\"Target Pile of Sand\")\naxes[1].set_xlabel(\"Sand Position\")\n\n# Plot transport plan with aesthetic lines\nfor i in range(len(pile_1_sorted)):\n    axes[2].plot([0, 1], [pile_1_sorted[i], pile_2_sorted[i]], color=\"purple\", alpha=0.7, lw=1)\n\naxes[2].scatter([0]*len(pile_1_sorted), pile_1_sorted, color=\"blue\", label=\"Initial Positions\", zorder=5)\naxes[2].scatter([1]*len(pile_2_sorted), pile_2_sorted, color=\"green\", label=\"Target Positions\", zorder=5)\n\naxes[2].set_title(\"Transport Plan\")\naxes[2].set_xticks([0, 1])\naxes[2].set_xticklabels([\"Initial\", \"Target\"])\naxes[2].set_xlim(-0.5, 1.5)\naxes[2].set_xlabel(\"Sand Pile\")\naxes[2].set_ylabel(\"Sand Position\")\naxes[2].legend()\n\n# Adjust layout and display\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFigure¬†2: Visualization of the sandpile transport problem, including initial and target distributions as well as the transport plan.\n\n\n\n\n\n\n\n\n4.1.2 The Modern Solution: Kantorovich‚Äôs Breakthrough\nLeonid Kantorovich revolutionized the field by allowing us to split and combine masses. His formulation looks like this:\n\\[\nW_p(P, Q) = \\left( \\inf_{J \\in \\mathcal{J}(P, Q)} \\int_{\\mathbb{R}^d \\times \\mathbb{R}^d} \\|x - y\\|^p \\, dJ(x, y) \\right)^{1/p}\n\\]\n\nJ(x, y): A joint distribution over (x, y) pairs. Think of J as specifying how much ‚Äúmass‚Äù at x under P is transported to y under Q.\nJ(P, Q): The set of all possible joint distributions J whose marginals are P and Q.\nMarginal Condition :\n\n\\(\\int J(x, y) \\, dy = P(x)\\), All mass at x under P is accounted for.\n\n\\(\\int J(x, y) \\, dx = Q(y)\\), All mass at y under Q is accounted for.\n\n\\(‚à•x - y‚à•^p\\) : The cost of transporting mass between x and y.\n\nInstead of finding a direct map, we now look for a ‚Äútransport plan‚Äù \\(J(x,y)\\) that tells us how much mass to move from each point \\(x\\) to each point \\(y\\). It‚Äôs like having a shipping manifest that details how much sand moves between any two locations.\nAn interesting point is the fact that the Kantorovich problems can be express a dual formula.\nIn this dual formulation, instead of finding a transport plan J, we find functions \\(\\phi(x)\\) and \\(\\psi(y)\\) that describe the ‚Äúpotential energy‚Äù of moving mass from P to Q. The supremum is taken over all measurable functions \\(\\psi\\) and \\(\\phi\\).\n\\[\\mathcal{W}_p(P, Q) = \\sup_{\\psi, \\phi} \\left( \\int_{R^ d}\\psi(y)\\,dQ(y) - \\int_{R^d}\\phi(x)\\,dP(x) \\right)\\]\nwhere \\(\\psi(y) - \\phi(x) \\leq \\|x - y\\|^p\\)\nSo, instead of finding a transport plan J, we find functions \\(\\phi(x)\\) and \\(\\psi(y)\\) that describe the ‚Äúpotential energy‚Äù of moving mass from distribution P to Q. These functions, often referred to as potentials or couplings, quantify the minimal cumulative cost required to transport mass from any point x in the source (P) to a point with zero potential energy in the target (Q), and vice versa. By optimizing over these potentials, we can efficiently compute the minimal cost of transporting all mass from P to Q.\nBut before seeing what does thies mean let‚Äôs look some special case."
  },
  {
    "objectID": "posts/2024-27-12-distribution-divergence/index.html#special-cases-and-practical-trick",
    "href": "posts/2024-27-12-distribution-divergence/index.html#special-cases-and-practical-trick",
    "title": "The Quest for understanding Distribution Distance: From Theory to Practice",
    "section": "4.2 Special Cases and Practical Trick",
    "text": "4.2 Special Cases and Practical Trick\n\n4.2.1 The One-Dimensional Miracle\nIn one dimension, something magical happens. The Wasserstein distance becomes surprisingly simple to compute:\n\\[\nW_p(P, Q) = \\left( \\int_{0}^1 \\left| F^{-1}(z) - G^{-1}(z) \\right|^p dz \\right)^{1/p}\n\\]\nBecome for \\(p=2\\) the simple : This is like comparing two distributions by aligning their quantiles and measuring the average distance between corresponding points. It‚Äôs particularly useful when working with simple univariate distributions.\n\nCode\n# Generate data\nx = np.linspace(0, 8, 200)\ndist1 = stats.norm.pdf(x, loc=3, scale=0.5)\ndist2 = stats.norm.pdf(x, loc=4, scale=0.7)\n\n# Create plot\nplt.figure(figsize=(10, 6))\nplt.plot(x, dist1, label='Distribution P', color='#8884d8')\nplt.plot(x, dist2, label='Distribution Q', color='#82ca9d')\n\n# Add quantile lines\nquantile_points = [0.25, 0.5, 0.75]\nfor q in quantile_points:\n    q1 = stats.norm.ppf(q, loc=3, scale=0.5)\n    q2 = stats.norm.ppf(q, loc=4, scale=0.7)\n    plt.vlines(q1, 0, stats.norm.pdf(q1, loc=3, scale=0.5), \n               colors='#8884d8', linestyles='--', alpha=0.5)\n    plt.vlines(q2, 0, stats.norm.pdf(q2, loc=4, scale=0.7), \n               colors='#82ca9d', linestyles='--', alpha=0.5)\n    plt.plot([q1, q2], \n            [stats.norm.pdf(q1, loc=3, scale=0.5), \n             stats.norm.pdf(q2, loc=4, scale=0.7)], \n            'gray', alpha=0.3)\n\nplt.xlabel('x')\nplt.ylabel('Density')\nplt.title('Wasserstein Distance Visualization')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFigure¬†3: Visualization of the Wasserstein Distance between two normal distributions with annotated quantile lines.\n\n\n\n\n\n\n\n\n4.2.2 The Gaussian Case: A Beautiful Formula\nWhen comparing two Gaussian distributions, we get an especially elegant result:\n\\[\nW_2^2(P, Q) = \\|\\mu_1 - \\mu_2\\|^2 + \\text{trace}(\\Sigma_1 + \\Sigma_2 - 2(\\Sigma_1^{1/2} \\Sigma_2 \\Sigma_1^{1/2})^{1/2})\n\\]\n\nDistance between the means: \\(\\|\\mu_1 - \\mu_2\\|_2\\) Measures the Euclidean distance between the means of two distributions.\nTrace: \\(\\text{Tr}(\\Sigma_1 - \\Sigma_2)^2\\) Measures differences in the covariance structures, where \\(\\Sigma_1\\) and \\(\\Sigma_2\\) are the covariance matrices of the respective distributions.\n\nThis formula neatly separates the difference between means from the difference in spread and shape."
  },
  {
    "objectID": "posts/2024-27-12-distribution-divergence/index.html#making-it-practical-the-sinkhorn-algorithm",
    "href": "posts/2024-27-12-distribution-divergence/index.html#making-it-practical-the-sinkhorn-algorithm",
    "title": "The Quest for understanding Distribution Distance: From Theory to Practice",
    "section": "4.3 Making It Practical: The Sinkhorn Algorithm",
    "text": "4.3 Making It Practical: The Sinkhorn Algorithm\nIn real-world applications, when \\(P\\) and \\(Q\\) are discrete distribution, computing the exact Wasserstein distance can be computationally intensive. Enter the Sinkhorn algorithm, which adds a touch of entropy regularization:\n\\[\nW_p^\\lambda(P, Q) = \\inf_{J \\in \\mathcal{J}(P, Q)} \\int_{\\mathbb{R}^d \\times \\mathbb{R}^d} \\|x - y\\|^p dJ(x, y) + \\lambda \\int_{\\mathbb{R}^d \\times \\mathbb{R}^d} J(x, y) \\log J(x, y)\n\\]\nThis regularization makes the computation much more tractable while still preserving the essential properties of the Wasserstein distance.\n\n\n\n\n\n\nMistral Nemo Explain üë®üèª‚Äçüè´\n\n\n\n\n\n\nEntropy Regularization: The key idea behind entropy regularization is to add a term proportional to the negative entropy (or KL-divergence) of the transport plan J to the original optimization problem. This has two main effects:\n\nIt encourages J to be more ‚Äúuniform‚Äù or ‚Äúentropic,‚Äù making it easier to optimize.\nIt introduces a bias towards transports that are close to the uniform distribution, making the problem smoother and better suited for numerical methods like the Sinkhorn algorithm.\n\nSinkhorn Iterations: The Sinkhorn algorithm is an iterative method that finds an approximate solution to the entropy-regularized Wasserstein distance minimization problem. It works by alternating between updating two potential functions, \\(\\phi(x)\\) (for source distribution P) and \\(\\psi(y)\\) (for target distribution Q), while keeping track of a transport plan K that is row-wise and column-wise stochastic (i.e., each row and column sums to 1). Here‚Äôs how the algorithm progresses:\n\nInitialize potential functions \\(\\phi^{(0)}(x) = \\log P(x)\\) and \\(\\psi^{(0)}(y) = \\log Q(y)\\), or some initial guess.\nIterate the following steps until convergence:\n\nRow Update: Compute a new transport plan K by normalizing the row-wise product of potentials: \\[\nK_{ij} \\propto P_i Q_j e^{-\\|x_i - y_j\\|_p / \\lambda} e^{\\phi^{(t)}(x_i) + \\psi^{(t)}(y_j)}\n\\] where \\(P_i\\) and \\(Q_j\\) are the discrete probabilities for sources and targets, respectively.\nColumn Update: Normalize the columns of K to obtain an updated transport plan K‚Äô, and then compute new potentials: \\[\n\\phi^{(t+1)}(x_i) = \\log \\left( \\sum_j K'_{ij} e^{- \\|x_i - y_j\\|_p / \\lambda} e^{\\psi^{(t)}(y_j)} \\right)\n\\] \\[\n\\psi^{(t+1)}(y_j) = \\phi^{(t+1)}(x_0) + \\frac{\\|x_0 - y_j\\|_p}{\\lambda} - \\log Q_j\n\\] where \\(x_0\\) is an arbitrary point (often chosen as the mean or median of P).\n\n\nConvergence: The Sinkhorn algorithm is guaranteed to converge for any initial potentials, and the limit transport plan K satisfies a fixed-point equation that corresponds to the solution of the entropy-regularized Wasserstein distance minimization problem. Moreover, as \\(\\lambda \\to 0\\), the approximate solution converges to the exact solution of the original (unregularized) problem.\nAdvantages: The main advantages of using the Sinkhorn algorithm with entropy regularization are:\n\nIt provides an efficient and easy-to-implement method for computing approximations of the Wasserstein distance between discrete distributions.\nIt can handle large-scale problems, as it only requires solving a sequence of linear systems at each iteration.\nIt is robust to initialization and converges quickly in practice.\n\n\n\n\n\nTo resume: Given two distributions P and Q, the core idea of the Wasserstein distance is to find the ‚Äúleast-cost‚Äù way to transport mass from P to Q. This problem can be formulated either as a Monge map (direct transportation) or a Kantorovich potential (joint distribution).\nThe output is a scalar distance \\(\\mathcal{W}_p(\\textbf{P}, \\textbf{Q})\\) that quantifies the minimal cumulative cost of transporting all mass from P to Q, where costs are measured using the \\(L_p\\) norm. Optionally, an explicit transport plan or map can also be provided.\nThis formulation enables us to compare and analyze distributions based on their relative transportation costs, making it a powerful tool in various applications such as machine learning, shape analysis, and optimal transport theory. In the next part, we will explore practical algorithms for computing the Wasserstein distance between discrete distributions."
  },
  {
    "objectID": "posts/2024-27-12-distribution-divergence/index.html#optimal-transport-for-1d-distributions-finding-the-best-path",
    "href": "posts/2024-27-12-distribution-divergence/index.html#optimal-transport-for-1d-distributions-finding-the-best-path",
    "title": "The Quest for understanding Distribution Distance: From Theory to Practice",
    "section": "5.1 Optimal Transport for 1D Distributions: Finding the Best Path",
    "text": "5.1 Optimal Transport for 1D Distributions: Finding the Best Path\nIn this section, we explore how to compute the ‚Äúoptimal transport plan‚Äù between two 1D distributions \\(p(x)\\) and \\(q(x)\\).\nAs already explain, the concept of optimal transport lies at the heart of the Wasserstein distance, which measures the ‚Äúeffort‚Äù required to morph one distribution into another.\nThis is particularly useful for comparing probability distributions, whether as a loss function in machine learning or for creating monitoring metrics.\nLet‚Äôs explain what will happen in the code."
  },
  {
    "objectID": "posts/2024-27-12-distribution-divergence/index.html#first-stage-the-cost-matrix",
    "href": "posts/2024-27-12-distribution-divergence/index.html#first-stage-the-cost-matrix",
    "title": "The Quest for understanding Distribution Distance: From Theory to Practice",
    "section": "5.2 First Stage: The Cost Matrix",
    "text": "5.2 First Stage: The Cost Matrix\nTo determine the optimal transport plan, we first define a cost matrix \\(C\\). Each entry \\(C[i][j]\\) represents the cost of moving ‚Äúmass‚Äù from bin \\(i\\) of \\(p(x)\\) to bin \\(j\\) of \\(q(x)\\). For this example, we use the squared Euclidean distance between bins:\n\\[\nC[i][j] = (i - j)^2\n\\]\nThis choice ensures that movements over longer distances incur a higher cost, reflecting their physical or conceptual separation."
  },
  {
    "objectID": "posts/2024-27-12-distribution-divergence/index.html#second-stage-linear-programming-approach-for-finding-the-best-transport-plan.",
    "href": "posts/2024-27-12-distribution-divergence/index.html#second-stage-linear-programming-approach-for-finding-the-best-transport-plan.",
    "title": "The Quest for understanding Distribution Distance: From Theory to Practice",
    "section": "5.3 Second Stage : Linear Programming Approach for finding the best Transport Plan.",
    "text": "5.3 Second Stage : Linear Programming Approach for finding the best Transport Plan.\nThe optimal transport problem can be formulated as a linear programming (LP) problem. Our goal is to find a transport plan \\(T\\), where \\(T[i][j]\\) specifies the amount of mass to move from bin \\(i\\) of \\(p(x)\\) to bin \\(j\\) of \\(q(x)\\), such that:\n\nThe row sums of \\(T\\) equal \\(p\\), ensuring all mass in \\(p(x)\\) is transported: \\[\n\\sum_{j} T[i][j] = p[i] \\quad \\forall i\n\\]\nThe column sums of \\(T\\) equal \\(q\\), ensuring all mass in \\(q(x)\\) is received: \\[\n\\sum_{i} T[i][j] = q[j] \\quad \\forall j\n\\]\nThe total cost is minimized: \\[\n\\text{Minimize } \\sum_{i, j} T[i][j] \\cdot C[i][j]\n\\]\n\nWe use the scipy.optimize.linprog function to solve this LP problem. The result is a transport plan \\(T\\) that minimizes the total cost while satisfying the marginal constraints.\n\n\n\n\n\n\nPersonal introspection\n\n\n\nAs I write this, I‚Äôm thinking back to some of my courses on path-finding algorithms like A* and Dijkstra. Maybe there‚Äôs a link?"
  },
  {
    "objectID": "posts/2024-27-12-distribution-divergence/index.html#third-code-walkthrough",
    "href": "posts/2024-27-12-distribution-divergence/index.html#third-code-walkthrough",
    "title": "The Quest for understanding Distribution Distance: From Theory to Practice",
    "section": "5.4 Third : Code Walkthrough",
    "text": "5.4 Third : Code Walkthrough\nLet‚Äôs resume what we want to observe:\n\nThe Density Functions\nThe kernel density estimates (KDEs) of \\(p(x)\\) and \\(q(x)\\) give an intuitive view of the distributions.\nThe Cost Matrix\nThe heatmap of the cost matrix \\(C\\) illustrates the squared distances between bins. Higher values indicate a higher cost of transportation.\nThe Optimal Transport Plan\nBy visualized \\(T\\) as a heatmap we will shows the amounts of mass transported from each bin of \\(p(x)\\) to \\(q(x)\\).\n\nBelow are the results of applying the above concepts:\n\n\nCode\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import linprog\n\nnp.random.seed(42)\n\nnb_b = 50\n\np = np.random.rand(nb_b)\np = p / p.sum()  \n\nq = np.random.rand(nb_b)\nq = q / q.sum()  \n\n# Create the cost matrix C based on squared Euclidean distance\nbins = len(p)  \nC = np.square(np.subtract.outer(np.arange(bins), np.arange(bins)))  # Squared distance\n\n# Flatten distributions and cost matrix for linear programming\nc = C.flatten()  # Cost vector (flattened cost matrix)\nA_eq = np.zeros((2 * bins, bins * bins))  # Equality constraints matrix\nb_eq = np.concatenate([p, q])  # Marginal sums for p and q\n\n# Build the A_eq matrix to enforce the marginals for p and q\nfor i in range(bins):\n    A_eq[i, i * bins: (i + 1) * bins] = 1  # For row sums to p[i]\n    A_eq[i + bins, i::bins] = 1  # For column sums to q[i]\n\n# Solve the linear programming problem\nresult = linprog(c, A_eq=A_eq, b_eq=b_eq, method='highs')\n\n# Retrieve the transport plan matrix T from the solution\nT = result.x.reshape((bins, bins))\n\n\ndef plot_density(p, q):\n    _, (ax1, ax2) = plt.subplots(2, 1, figsize=(5, 5))\n    \n    sns.kdeplot(p, color='blue', ax=ax1, linewidth=2, bw_adjust=0.3)\n    ax1.set_title('p(x)', fontsize=14, fontweight='bold')\n    ax1.set_xlabel('Bins', fontsize=14)\n    ax1.set_ylabel('Density', fontsize=14)\n\n    sns.kdeplot(q, color='orange', ax=ax2, linewidth=2, bw_adjust=0.3)\n    ax2.set_title('q(x)', fontsize=14, fontweight='bold')\n    ax2.set_xlabel('Bins', fontsize=14)\n    ax2.set_ylabel('Density', fontsize=14)\n\n    plt.tight_layout()\n    plt.show()\n\n\ndef plot_cost_matrix(C, bins):\n    plt.figure(figsize=(5,5))\n    sns.heatmap(C, annot=False, fmt='.1f', cmap=sns.color_palette(\"light:blue\", as_cmap=True), cbar=True,\n                xticklabels=[f'{i+1}' for i in range(bins)],\n                yticklabels=[f'{i+1}' for i in range(bins)],\n                cbar_kws={'label': 'Squared Distance'})\n    plt.title('Cost Matrix: Squared Euclidean Distance', fontsize=14, fontweight='bold')\n    plt.xlabel('Bins of Distribution q', fontsize=14)\n    plt.ylabel('Bins of Distribution p', fontsize=14)\n    plt.tight_layout()\n    plt.show()\n\n\ndef plot_transport_plan(T, bins):\n    plt.figure(figsize=(5,5))\n    sns.heatmap(T, annot=False, fmt='.2f', cmap=sns.color_palette(\"light:blue\", as_cmap=True), cbar=True,\n                xticklabels=[f'{i+1}' for i in range(bins)],\n                yticklabels=[f'{i+1}' for i in range(bins)],\n                cbar_kws={'label': 'Transport Amount'})\n    plt.title(\"Optimal Transport Plan\", fontsize=14, fontweight='bold')\n    plt.xlabel(\"Bins of Distribution q\", fontsize=14)\n    plt.ylabel(\"Bins of Distribution p\", fontsize=14)\n    plt.tight_layout()\n    plt.show()\n\n\n\nCode\nplot_density(p, q)\n\nplot_cost_matrix(C, bins)\n\n\n\n\n\n\n\n\n\n\nFigure¬†4: Density plots of the two distributions p and q\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†5: Cost matrix based on squared Euclidean distance between bins of distributions p and q.\n\n\n\n\n\n\n\nCode\nplot_density(p, q)\n\nplot_transport_plan(T, bins)\n\n\n\n\n\n\n\n\n\n\nFigure¬†6: Density plots of the two distributions p and q\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†7: Optimal transport plan showing the transport amounts between bins of p and q.\n\n\n\n\n\n\nThe visualization of the optimal transport plan clearly shows how \\(p(x)\\) is ‚Äúredistributed‚Äù to align with \\(q(x)\\).\nEach nonzero element of T corresponds to a connection between \\(p[i]\\) and \\(q[j]\\), with the intensity reflecting the amount of mass transported.\nBut what‚Äôs the connection with the Wasserstein Distance ?\nHer, the optimal transport plan \\(T\\) allows us to compute the Wasserstein distance as the square root of the total transport cost:\n\\[\nW_2(p, q) = \\sqrt{\\sum_{i, j} T[i][j] \\cdot C[i][j]}\n\\]\nThis metric provides a robust way to compare distributions, considering both the location and ‚Äúeffort‚Äù to match their mass. It is particularly advantageous over other metrics like Kullback-Leibler divergence for distributions with non-overlapping supports.\nOf course, it is a simple and non optimal exemple, and algorythm like Sinkhorn algorithm will work better."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "A propos de moi",
    "section": "",
    "text": "üåü Ce que j‚Äôapprends au quotidien\nPlut√¥t qu‚Äôune liste classique fa√ßon CV, voici les domaines qui attisent ma curiosit√© et enrichissent mes comp√©tences au fil des jours :\n\nüöÄ D√©veloppement logiciel\n\nGit : l‚Äôincontournable pour une collaboration efficace.\n\nApprofondir Python et explorer Go : deux langages aux forces compl√©mentaires pour les projets data et syst√®mes performants.\n\nMa√Ætriser les structures de donn√©es pour optimiser les algorithmes.\n\nComprendre les syst√®mes distribu√©s, le fondement des applications modernes.\n\nExplorer les outils et concepts de ML Ops : Docker, Kubernetes, CI/CD pour automatiser et fiabiliser les pipelines d‚Äôapprentissage automatique.\n\nApprofondir la programmation orient√©e objet et fonctionnelle pour structurer et simplifier les solutions complexes.\n\nMaitriser les design patterns (Strategy, Factory) pour concevoir des architectures robustes et √©volutives.\n\n\n\nüî¨ Science\n\nRenforcer mes bases en alg√®bre lin√©aire, essentielle pour l‚Äôapprentissage automatique.\n\nApprofondir le calcul diff√©rentiel et int√©gral pour mod√©liser et optimiser des syst√®mes complexes.\n\nMa√Ætriser les probabilit√©s et les distributions de probabilit√© pour g√©rer l‚Äôincertitude dans les donn√©es.\n\nAffiner mes comp√©tences en statistiques, un pilier incontournable pour un DS.\n\nExplorer la g√©om√©trie, o√π les math√©matiques rencontrent la visualisation.\n\nComprendre la th√©orie de l‚Äôinformation : entropie, compression, et transmission des donn√©es.\n\nEtudier des techniques d‚Äôoptimisation math√©matique.\n√âtudier la th√©orie des graphes, cl√© pour mod√©liser les relations et r√©seaux complexes."
  }
]
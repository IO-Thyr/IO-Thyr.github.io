<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tancrede Hayer">
<meta name="dcterms.date" content="2024-12-27">

<title>The Quest for understanding Distribution Distance: From Theory to Practice – I/O</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../flamme_personnage.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-2486e1f0a3ee9ee1fc393803a1361cdb.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-0437d84738b223c6628de6fd3abdae78.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>

      .quarto-title-block .quarto-title-banner h1,
      .quarto-title-block .quarto-title-banner h2,
      .quarto-title-block .quarto-title-banner h3,
      .quarto-title-block .quarto-title-banner h4,
      .quarto-title-block .quarto-title-banner h5,
      .quarto-title-block .quarto-title-banner h6
      {
        color: black;
      }

      .quarto-title-block .quarto-title-banner {
        color: black;
background: #f0f3f5;
      }
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../disk.jpeg" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">I/O</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://quarto.org/docs/websites"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://quarto.org/docs/websites"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.youtube.com/@essience"> <i class="bi bi-youtube" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">The Quest for understanding Distribution Distance: From Theory to Practice</h1>
            <p class="subtitle lead">Understanding the Mathematics (but not too many) of Distribution Comparison</p>
                                <div class="quarto-categories">
                <div class="quarto-category">Information Theory</div>
                <div class="quarto-category">Math</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Tancrede Hayer </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 27, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#why-do-we-care-about-piles-of-sand" id="toc-why-do-we-care-about-piles-of-sand" class="nav-link active" data-scroll-target="#why-do-we-care-about-piles-of-sand"><span class="header-section-number">1</span> Why do we care about piles of sand ?</a></li>
  <li><a href="#the-kullback-leibler-and-jensen-shannon-divergence" id="toc-the-kullback-leibler-and-jensen-shannon-divergence" class="nav-link" data-scroll-target="#the-kullback-leibler-and-jensen-shannon-divergence"><span class="header-section-number">2</span> The Kullback-Leibler and Jensen-Shannon divergence</a>
  <ul class="collapse">
  <li><a href="#the-kullback-leibler-divergence" id="toc-the-kullback-leibler-divergence" class="nav-link" data-scroll-target="#the-kullback-leibler-divergence"><span class="header-section-number">2.1</span> The Kullback-Leibler divergence</a></li>
  <li><a href="#jensenshannon-divergence" id="toc-jensenshannon-divergence" class="nav-link" data-scroll-target="#jensenshannon-divergence"><span class="header-section-number">2.2</span> Jensen–Shannon Divergence</a></li>
  <li><a href="#a-picture-is-worth-a-thousand-words" id="toc-a-picture-is-worth-a-thousand-words" class="nav-link" data-scroll-target="#a-picture-is-worth-a-thousand-words"><span class="header-section-number">2.3</span> A picture is worth a thousand words</a></li>
  </ul></li>
  <li><a href="#a-brief-pause" id="toc-a-brief-pause" class="nav-link" data-scroll-target="#a-brief-pause"><span class="header-section-number">3</span> A Brief Pause</a></li>
  <li><a href="#the-wasserstein-distance" id="toc-the-wasserstein-distance" class="nav-link" data-scroll-target="#the-wasserstein-distance"><span class="header-section-number">4</span> The Wasserstein distance</a>
  <ul class="collapse">
  <li><a href="#the-mathematical-journey" id="toc-the-mathematical-journey" class="nav-link" data-scroll-target="#the-mathematical-journey"><span class="header-section-number">4.1</span> The Mathematical Journey</a>
  <ul class="collapse">
  <li><a href="#the-classical-approach-monges-vision" id="toc-the-classical-approach-monges-vision" class="nav-link" data-scroll-target="#the-classical-approach-monges-vision"><span class="header-section-number">4.1.1</span> The Classical Approach: Monge’s Vision</a></li>
  <li><a href="#the-modern-solution-kantorovichs-breakthrough" id="toc-the-modern-solution-kantorovichs-breakthrough" class="nav-link" data-scroll-target="#the-modern-solution-kantorovichs-breakthrough"><span class="header-section-number">4.1.2</span> The Modern Solution: Kantorovich’s Breakthrough</a></li>
  </ul></li>
  <li><a href="#special-cases-and-practical-trick" id="toc-special-cases-and-practical-trick" class="nav-link" data-scroll-target="#special-cases-and-practical-trick"><span class="header-section-number">4.2</span> Special Cases and Practical Trick</a>
  <ul class="collapse">
  <li><a href="#the-one-dimensional-miracle" id="toc-the-one-dimensional-miracle" class="nav-link" data-scroll-target="#the-one-dimensional-miracle"><span class="header-section-number">4.2.1</span> The One-Dimensional Miracle</a></li>
  <li><a href="#the-gaussian-case-a-beautiful-formula" id="toc-the-gaussian-case-a-beautiful-formula" class="nav-link" data-scroll-target="#the-gaussian-case-a-beautiful-formula"><span class="header-section-number">4.2.2</span> The Gaussian Case: A Beautiful Formula</a></li>
  </ul></li>
  <li><a href="#making-it-practical-the-sinkhorn-algorithm" id="toc-making-it-practical-the-sinkhorn-algorithm" class="nav-link" data-scroll-target="#making-it-practical-the-sinkhorn-algorithm"><span class="header-section-number">4.3</span> Making It Practical: The Sinkhorn Algorithm</a></li>
  </ul></li>
  <li><a href="#problem-solving-how-to-find-the-optimal-path-for-1d-distribution" id="toc-problem-solving-how-to-find-the-optimal-path-for-1d-distribution" class="nav-link" data-scroll-target="#problem-solving-how-to-find-the-optimal-path-for-1d-distribution"><span class="header-section-number">5</span> Problem Solving : How to find the optimal path for 1D distribution ?</a>
  <ul class="collapse">
  <li><a href="#optimal-transport-for-1d-distributions-finding-the-best-path" id="toc-optimal-transport-for-1d-distributions-finding-the-best-path" class="nav-link" data-scroll-target="#optimal-transport-for-1d-distributions-finding-the-best-path"><span class="header-section-number">5.1</span> Optimal Transport for 1D Distributions: Finding the Best Path</a></li>
  <li><a href="#first-stage-the-cost-matrix" id="toc-first-stage-the-cost-matrix" class="nav-link" data-scroll-target="#first-stage-the-cost-matrix"><span class="header-section-number">5.2</span> First Stage: The Cost Matrix</a></li>
  <li><a href="#second-stage-linear-programming-approach-for-finding-the-best-transport-plan." id="toc-second-stage-linear-programming-approach-for-finding-the-best-transport-plan." class="nav-link" data-scroll-target="#second-stage-linear-programming-approach-for-finding-the-best-transport-plan."><span class="header-section-number">5.3</span> Second Stage : Linear Programming Approach for finding the best Transport Plan.</a></li>
  <li><a href="#third-code-walkthrough" id="toc-third-code-walkthrough" class="nav-link" data-scroll-target="#third-code-walkthrough"><span class="header-section-number">5.4</span> Third : Code Walkthrough</a></li>
  </ul></li>
  <li><a href="#this-is-the-end" id="toc-this-is-the-end" class="nav-link" data-scroll-target="#this-is-the-end"><span class="header-section-number">6</span> This is the end</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">7</span> References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">





<p>Have you ever wondered how we measure how “far apart” two distributions are in data science? It’s not as simple as it might seem, but don’t worry – we’ll keep the math tame and focus on intuition.</p>
<p>By the end of this reading, you will have a better understanding of optimal transport theory and how it helps us in data science and model monitoring.</p>
<section id="why-do-we-care-about-piles-of-sand" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Why do we care about piles of sand ?</h1>
<p>An important topic in machine learning (and math in general) is to find a useful way to measure “distance” between pairs of distributions.</p>
<p>Among their properties, symmetry and the triangle inequality play central roles in ensuring that these measures behave intuitively and are mathematically robust.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Why ?">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why ?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Symmetry is distance metrics between two distributions <code>P</code> and <code>Q</code> ensures their distance remains unchanged regardless of which distribution is considered first <span class="math inline">\(d(P, Q) = d(Q, P)\)</span>.</p>
<p>This symmetry allows for a fair and consistent comparison, as the direction of measurement doesn’t affect the outcome.</p>
<p>The triangle inequality property guarantees that the distance between any two points (distributions) is always less than or equal to the sum of the distances to a third point. For three distributions <code>P</code>, <code>Q</code>, and <code>R</code>, this is expressed as <span class="math inline">\(d(P, R) ≤ d(P, Q) + d(Q, R)\)</span>.</p>
<p>This property ensures that our distance metrics align with the intuitive notion of “shortest path.”</p>
</div>
</div>
</div>
</section>
<section id="the-kullback-leibler-and-jensen-shannon-divergence" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> The Kullback-Leibler and Jensen-Shannon divergence</h1>
<section id="the-kullback-leibler-divergence" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="the-kullback-leibler-divergence"><span class="header-section-number">2.1</span> The Kullback-Leibler divergence</h2>
<p>Imagine you’re in a bustling city, and you have two maps, <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>, trying to guide you through the streets. Now, we want to know which map is more reliable when navigating from point A to B. Here comes KL divergence, to help us out!</p>
<p>The KL Divergence measures how one probability distribution <span class="math inline">\(Q\)</span> diverges from another, <span class="math inline">\(P\)</span> :</p>
<p><span class="math display">\[
D_{KL}(P \| Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}
\]</span></p>
<p>For continuous distributions it is :</p>
<p><span class="math display">\[
D_{KL}(P \| Q) = \int_{-\infty}^{\infty} p(x) \log \frac{p(x)}{q(x)} dx
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled" title="Concreate example">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Concreate example
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Let’s make this concrete with a simple city layout having three key locations: Home (H), Work (W), and Shopping Mall (S).</p>
<p>The true map <span class="math inline">\(P\)</span> shows these probabilities of successful routes:</p>
<ul>
<li><span class="math inline">\(P(H→W) = 0.5\)</span> (a direct highway route)</li>
<li><span class="math inline">\(P(H→S) = 0.3\)</span> (a clear urban route)</li>
<li><span class="math inline">\(P(H→M) = 0.2\)</span> (a residential path)</li>
</ul>
<p>While the approximate map <span class="math inline">\(Q\)</span> shows:</p>
<ul>
<li><span class="math inline">\(Q(H→W) = 0.4\)</span> (underestimates highway access)</li>
<li><span class="math inline">\(Q(H→S) = 0.4\)</span> (overestimates this route)</li>
<li><span class="math inline">\(Q(H→M) = 0.2\)</span> (correctly estimates this one)</li>
</ul>
<p>Let’s calculate this for our city example:</p>
<ul>
<li>For the Work route: <span class="math display">\[P(H→W) * log(P(H→W)/Q(H→W)) = 0.5 * log(0.5/0.4) ≈ 0.048\]</span></li>
<li>For the Shopping route: <span class="math display">\[P(H→S) * log(P(H→S)/Q(H→S)) = 0.3 * log(0.3/0.4) ≈ -0.037\]</span></li>
<li>For the Mall route: <span class="math display">\[P(H→M) * log(P(H→M)/Q(H→M)) = 0.2 * log(0.2/0.2) ≈ 0\]</span></li>
</ul>
<p>Total KL divergence = 0.048 -0.037 + 0 = 0.011</p>
<p>This means using map Q instead of P results in about 1.1% inefficiency in navigation. In other words, you’ll spend about 1.1% more time or distance traveling when using the approximate map Q compared to the true map P.</p>
</div>
</div>
</div>
<p>In this cityscape analogy, the KL divergence measures how much extra walking you’d do if you used map <span class="math inline">\(Q\)</span> instead of the true map <span class="math inline">\(P\)</span>. When following directions from <span class="math inline">\(Q\)</span>, the additional distance traveled is proportional to <span class="math inline">\(D_{KL}(P \| Q)\)</span>. It’s like a penalty for when <span class="math inline">\(Q\)</span> underestimates or misrepresents the probability of going through certain streets compared to <span class="math inline">\(P\)</span>.</p>
<p>The KL divergence quantifies the inefficiency of assuming that the distribution <span class="math inline">\(Q\)</span> approximates the true distribution <span class="math inline">\(P\)</span>. When encoding data points sampled from <span class="math inline">\(P\)</span> using a code optimized for <span class="math inline">\(Q\)</span>, the additional coding length is proportional to <span class="math inline">\(D_{KL}(P \| Q)\)</span>.</p>
<p>Now consider, for example, the log term <span class="math inline">\(\log \frac{P(x)}{Q(x)}\)</span>. If <span class="math inline">\(Q\)</span> suggests you go down an alley while <span class="math inline">\(P\)</span> shows it’s a dead end (i.e., <span class="math inline">\(Q(x)\)</span> approaches zero but <span class="math inline">\(P(x)\)</span> doesn’t), that’s when our navigator (KL divergence) really starts warning you, “Watch out! You’re about to take a long detour!”</p>
<p>The log term, <span class="math inline">\(\log \frac{P(x)}{Q(x)}\)</span>, acts as a penalty for underestimating probabilities in <span class="math inline">\(Q\)</span> compared to <span class="math inline">\(P\)</span>.</p>
<p>KL divergence heavily penalizes cases where <span class="math inline">\(Q(x)\)</span> approaches zero while <span class="math inline">\(P(x)\)</span> remains non-zero.</p>
<p>Before continuing let’s keep in mind the special properties of KL divergence through this lens:</p>
<ol type="1">
<li><p><strong>Non-Negativity</strong>: <span class="math inline">\(D_{KL}(P \| Q) \geq 0\)</span> - Using either map will never lead you astray; at worst, they’ll guide you equally well (when <span class="math inline">\(P = Q\)</span>).</p></li>
<li><p><strong>Zero Minimum</strong>: <span class="math inline">\(D_{KL}(P \| Q) = 0 \iff P = Q\)</span> - The only way our navigator won’t make you walk extra is if both maps are the same.</p></li>
<li><p><strong>Asymmetric</strong>: <span class="math inline">\(D_{KL}(P \| Q) \neq D_{KL}(Q \| P)\)</span> - Our navigator isn’t biased ; he’ll give different walking distance estimates depending on which map you’re using as your reference.</p></li>
</ol>
</section>
<section id="jensenshannon-divergence" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="jensenshannon-divergence"><span class="header-section-number">2.2</span> Jensen–Shannon Divergence</h2>
<p>Continuing our city analogy but now, instead of blindly following one or the other, you want to find a balanced and reliable way to combine them for your journey from point A to B. Here’s come the Jensen-Shannon (JS) divergence.</p>
<p>The JS Divergence is a symmetrized and smoother version of the KL Divergence, defined as:</p>
<p><span class="math display">\[
D_{JS}(P \| Q) = \frac{1}{2} D_{KL}(P \| M) + \frac{1}{2} D_{KL}(Q \| M), \text{ where } M = \frac{1}{2}(P + Q)
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled" title="Concreate example">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Concreate example
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Detailed Calculation of Jensen-Shannon Divergence</strong></p>
<p>Let’s work through a detailed calculation using our city navigation example to better understand how JS divergence works in practice.</p>
<ul>
<li>Step 1: Calculate M</li>
</ul>
<p>Given our probability distributions: P = [0.5, 0.3, 0.2] (true map) Q = [0.4, 0.4, 0.2] (approximate map)</p>
<p>The average distribution M is calculated as:</p>
<p><span class="math display">\[
M = \frac{1}{2}(P + Q) = \left[\frac{0.5 + 0.4}{2}, \frac{0.3 + 0.4}{2}, \frac{0.2 + 0.2}{2}\right] = [0.45, 0.35, 0.2]
\]</span></p>
<ul>
<li>Step 2: Calculate D_KL(P||M)</li>
</ul>
<p>The KL divergence from M to P is calculated as:</p>
<p><span class="math display">\[
D_{KL}(P \parallel M) = \sum_{i} P(i) \cdot \log\left(\frac{P(i)}{M(i)}\right)
\]</span></p>
<p>Let’s calculate each term:</p>
<ol type="1">
<li><p>For H→W route: <span class="math display">\[
P(0.5) \cdot \log\left(\frac{0.5}{0.45}\right) = 0.5 \cdot \log(1.1111) = 0.5 \cdot 0.0458 = 0.0229
\]</span></p></li>
<li><p>For H→S route: <span class="math display">\[
P(0.3) \cdot \log\left(\frac{0.3}{0.35}\right) = 0.3 \cdot \log(0.8571) = 0.3 \cdot (-0.0669) = -0.0201
\]</span></p></li>
<li><p>For H→M route: <span class="math display">\[
P(0.2) \cdot \log\left(\frac{0.2}{0.2}\right) = 0.2 \cdot \log(1) = 0
\]</span></p></li>
<li><p>Sum <span class="math display">\[
D_{KL}(P \parallel M) = 0.0229 - 0.0201 + 0 = 0.0028
\]</span></p></li>
</ol>
<ul>
<li>Step 3: Calculate D_KL(Q||M)</li>
</ul>
<p>Similarly for Q:</p>
<p><span class="math display">\[
D_{KL}(Q \parallel M) = \sum_{i} Q(i) \cdot \log\left(\frac{Q(i)}{M(i)}\right)
\]</span></p>
<ol type="1">
<li><p>For H→W route: <span class="math display">\[
Q(0.4) \cdot \log\left(\frac{0.4}{0.45}\right) = 0.4 \cdot \log(0.8889) = 0.4 \cdot (-0.0493) = -0.0197
\]</span></p></li>
<li><p>For H→S route: <span class="math display">\[
Q(0.4) \cdot \log\left(\frac{0.4}{0.35}\right) = 0.4 \cdot \log(1.1429) = 0.4 \cdot 0.0571 = 0.0228
\]</span></p></li>
<li><p>For H→M route: <span class="math display">\[
Q(0.2) \cdot \log\left(\frac{0.2}{0.2}\right) = 0.2 \cdot \log(1) = 0
\]</span></p></li>
<li><p>Sum: <span class="math display">\[
D_{KL}(Q \parallel M) = -0.0197 + 0.0228 + 0 = 0.0031
\]</span></p></li>
</ol>
<ul>
<li>Step 4: Calculate D_JS(P||Q)</li>
</ul>
<p>The final JS divergence is calculated as:</p>
<p><span class="math display">\[
D_{JS}(P \parallel Q) = \frac{1}{2} D_{KL}(P \parallel M) + \frac{1}{2} D_{KL}(Q \parallel M)
\]</span></p>
<p>Substituting our values:</p>
<p><span class="math display">\[
D_{JS}(P \parallel Q) = \frac{1}{2}(0.0028) + \frac{1}{2}(0.0031) = 0.0014 + 0.00155 = 0.00295
\]</span></p>
<p>The small JS divergence value (0.00295) indicates that our two maps P and Q are fairly similar in their routing suggestions. This makes sense because:</p>
<ol type="1">
<li>They completely agree on the probability of the residential path (H→M: 0.2)</li>
<li>They only slightly disagree on the highway route (0.5 vs 0.4)</li>
<li>Their difference in the urban route (0.3 vs 0.4) is moderate</li>
</ol>
<p>This numerical example shows how JS divergence provides a symmetric and bounded measure of the difference between two probability distributions, making it particularly useful for comparing different routing strategies or maps.</p>
</div>
</div>
</div>
<p>The JS divergence finds a middle ground between maps <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> by averaging them into a new map <span class="math inline">\(M = \frac{1}{2}(P + Q)\)</span>. It then measures how different this average route is from each original one, giving equal weight to both.</p>
<p>By combining and averaging the maps, our navigator (JS divergence) ensures a more stable and symmetric measure of their differences. This interpolation also helps when one map suggests going through a street that the other doesn’t – instead of getting stuck, our navigator finds a sensible alternative route.</p>
<p>Unlike KL divergence, the JS divergence measures the similarity of two distributions by incorporating their average, <span class="math inline">\(M\)</span>. By interpolating between <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>, JS divergence ensures a bounded and symmetric measure. This interpolation resolves issues of infinite divergence where <span class="math inline">\(P(x) &gt; 0\)</span> but <span class="math inline">\(Q(x) = 0\)</span>.</p>
<p>And let’s revisit the special properties of JS divergence using this city analogy:</p>
<ol type="1">
<li><p><strong>Symmetric</strong>: <span class="math inline">\(D_{JS}(P \| Q) = D_{JS}(Q \| P)\)</span> - Our compromising navigator gives equal importance to both maps and provides a balanced measure of their differences, ensuring you’re not favoring one over the other.</p></li>
<li><p><strong>Bounded</strong>: <span class="math inline">\(0 \leq D_{JS}(P \| Q) \leq 1\)</span> - The navigator’s score will always fall within a reasonable range, making it easier for you to gauge how similar or different the two maps are in guiding your journey.</p></li>
<li><p><strong>Smooth</strong>: Handles zero probabilities gracefully compared to KL Divergence - When one map suggests going down a street that the other doesn’t (or vice versa), our navigator won’t get flustered or lost. She’ll find an alternative route, making sure you still reach your destination with minimal d</p></li>
</ol>
</section>
<section id="a-picture-is-worth-a-thousand-words" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="a-picture-is-worth-a-thousand-words"><span class="header-section-number">2.3</span> A picture is worth a thousand words</h2>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> linprog</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> kl_divergence(p, q):</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> np.array(p)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    q <span class="op">=</span> np.array(q)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">sum</span>(np.where(p <span class="op">!=</span> <span class="dv">0</span>, p <span class="op">*</span> np.log(p <span class="op">/</span> q), <span class="dv">0</span>))</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> js_divergence(p, q):</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> np.array(p)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    q <span class="op">=</span> np.array(q)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> (p <span class="op">+</span> q)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    kl_mq <span class="op">=</span> kl_divergence(m, q)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    kl_mp <span class="op">=</span> kl_divergence(m, p)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> (kl_mq <span class="op">+</span> kl_mp)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Define two distributions</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> np.exp(<span class="op">-</span>x<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi)  <span class="co"># Normal distribution, mean=0, std=1</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> np.exp(<span class="op">-</span>(x <span class="op">-</span> <span class="dv">1</span>)<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi)  <span class="co"># Normal distribution, mean=1, std=1</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalize distributions</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>p <span class="op">/=</span> np.<span class="bu">sum</span>(p)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>q <span class="op">/=</span> np.<span class="bu">sum</span>(q)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute divergences and their components</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>kl_pq <span class="op">=</span> kl_divergence(p, q)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>kl_qp <span class="op">=</span> kl_divergence(q, p)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>js_pq <span class="op">=</span> js_divergence(p, q)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> (p <span class="op">+</span> q)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>kl_mq <span class="op">=</span> kl_divergence(m, q)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>kl_mp <span class="op">=</span> kl_divergence(m, p)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualization</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>(style<span class="op">=</span><span class="st">"whitegrid"</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Original distributions P and Q</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>plt.plot(x, p, label<span class="op">=</span><span class="st">"P: N(0,1)"</span>, color<span class="op">=</span><span class="st">"blue"</span>)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>plt.plot(x, q, label<span class="op">=</span><span class="st">"Q: N(1,1)"</span>, color<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="co"># KL(P || Q) component: P * log(P / Q)</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>kl_pq_component <span class="op">=</span> np.where(p <span class="op">!=</span> <span class="dv">0</span>, p <span class="op">*</span> np.log(p <span class="op">/</span> q), <span class="dv">0</span>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>plt.plot(x, kl_pq_component, label<span class="op">=</span><span class="ss">f"KL(P || Q): </span><span class="sc">{</span>kl_pq<span class="sc">:.4f}</span><span class="ss">"</span>, color<span class="op">=</span><span class="st">"green"</span>, linestyle<span class="op">=</span><span class="st">"--"</span>)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="co"># KL(Q || P) component: Q * log(Q / P)</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>kl_qp_component <span class="op">=</span> np.where(q <span class="op">!=</span> <span class="dv">0</span>, q <span class="op">*</span> np.log(q <span class="op">/</span> p), <span class="dv">0</span>)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>plt.plot(x, kl_qp_component, label<span class="op">=</span><span class="ss">f"KL(Q || P): </span><span class="sc">{</span>kl_qp<span class="sc">:.4f}</span><span class="ss">"</span>, color<span class="op">=</span><span class="st">"purple"</span>, linestyle<span class="op">=</span><span class="st">"--"</span>)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="co"># JS(P || Q) components: 0.5 * (P * log(P / M) + Q * log(Q / M))</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>js_pq_components <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> (np.where(p <span class="op">!=</span> <span class="dv">0</span>, p <span class="op">*</span> np.log(p <span class="op">/</span> m), <span class="dv">0</span>) <span class="op">+</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>                         np.where(q <span class="op">!=</span> <span class="dv">0</span>, q <span class="op">*</span> np.log(q <span class="op">/</span> m), <span class="dv">0</span>))</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>plt.plot(x, js_pq_components, label<span class="op">=</span><span class="ss">f"JS(P || Q): </span><span class="sc">{</span>js_pq<span class="sc">:.4f}</span><span class="ss">"</span>, color<span class="op">=</span><span class="st">"orange"</span>, linestyle<span class="op">=</span><span class="st">":"</span>)</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"KL and JS Divergence Components Visualization"</span>)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Probability Density / Component"</span>)</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="cell-fig-one" class="cell quarto-layout-panel" data-execution_count="2" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="cell-output cell-output-display quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div id="fig-one" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-one-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-one-output-1.png" width="982" height="529" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-one-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: KL and JS Divergence Components Visualization
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="What is divergence components ?">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What is divergence components ?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The divergence components represent the point-wise contributions to the overall divergence measures between distributions P and Q.<br>
The area under each component curve equals the total divergence measure. Locations where components peak indicate regions where the distributions differ most substantially, providing insight into local distribution differences.</p>
</div>
</div>
</div>
<p>The plot visualizes the comparison between Kullback-Leibler (KL) and Jensen-Shannon (JS) divergences for two normal distributions <span class="math inline">\(PN(0,1)\)</span> and <span class="math inline">\(QN(1,1)\)</span>.</p>
<p>The blue and red curves show the original distributions, while the green and purple dashed lines represent the KL divergence components <span class="math inline">\(KL(P||Q)\)</span> and <span class="math inline">\(KL(Q||P)\)</span> respectively, demonstrating KL’s asymmetry.</p>
<p>The orange dotted line shows the JS divergence component, which is symmetric and combines information from both distributions through their midpoint M.</p>
<p>The legend includes numerical values for each divergence measure, with <span class="math inline">\(KL(P||Q) ≠ KL(Q||P)\)</span>, illustrating KL’s asymmetric nature, while JS maintains a single value representing the symmetric difference between the distributions.</p>
</section>
</section>
<section id="a-brief-pause" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> A Brief Pause</h1>
<p>While KL and JS divergences focus on comparing the probabilistic structure of two distributions, they fail to capture spatial or geometric differences between distributions. This is especially important when comparing datasets where the “location” of the mass matters as much as the overall distribution.</p>
<p>Enter the <strong>Wasserstein Distance</strong>, a metric that builds upon the idea of moving probability mass between distributions. By framing the comparison in terms of transportation costs, Wasserstein Distance offers a more comprehensive perspective. It measures not only the discrepancy in probabilities but also the effort required to align distributions spatially. This makes it particularly valuable in modern machine learning applications like Generative Adversarial Networks (GANs), domain adaptation, and shape analysis.</p>
<p>In the sections that follow, we will delve deeper into the Wasserstein Distance, exploring its mathematical foundation, intuitive interpretations, and practical applications.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
To be more clear
</div>
</div>
<div class="callout-body-container callout-body">
<p>While KL and JS divergences can be computed using simple pointwise operations (e.g., division, multiplication, and addition) on the probability density functions (PDFs), followed by integration or summation. In contrast, the Wasserstein Distance requires solving an optimization problem, typically using the Monge-Kantorovich mass transfer theorem or related techniques like the Sinkhorn algorithm (This will be our next story, so be ready). This makes it more computationally involved.</p>
</div>
</div>
</section>
<section id="the-wasserstein-distance" class="level1 page-columns page-full" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> The Wasserstein distance</h1>
<p>Imagine you’re a data scientist faced with an intriguing challenge: how do you measure the similarity between two different distributions? Perhaps you’re comparing the distribution of customer spending patterns before and after a marketing campaign, or maybe you’re evaluating how well a generative model captures the essence of real data.</p>
<p>Enter the Wasserstein distance – a powerful mathematical tool that approaches this problem with an elegant metaphor: imagine your distributions as piles of earth, and you need to figure out the most efficient way to reshape one pile into the other.</p>
<section id="the-mathematical-journey" class="level2 page-columns page-full" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="the-mathematical-journey"><span class="header-section-number">4.1</span> The Mathematical Journey</h2>
<section id="the-classical-approach-monges-vision" class="level3 page-columns page-full" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="the-classical-approach-monges-vision"><span class="header-section-number">4.1.1</span> The Classical Approach: Monge’s Vision</h3>
<p>Gaspard Monge, an 18th-century French mathematician, first formulated this problem of optimal transportation who will later be express in terms of moving earth (hence the nickname “Earth Mover’s Distance” or “Monge-Kantorovich problem”). His formulation is the Wasserstein distance for <span class="math inline">\(p=1\)</span>:</p>
<p><span class="math display">\[
W_p(P, Q) = \left( \inf_{T} \int_{\mathbb{R}^d} \|x - T(x)\|^p \, dP(x) \right)^{1/p}
\]</span></p>
<p><span class="math display">\[W_p(μ, ν) = \left(\inf_{\gamma \in \Gamma(\mu, \nu)} \int_{X \times Y} d(x,y)^p d\gamma(x,y)\right)^{1/p}\]</span></p>
<ul>
<li>A transport map <span class="math inline">\(T(x)\)</span> moves mass at point <span class="math inline">\(x\)</span> under distribution <span class="math inline">\(P\)</span> to some point under distribution <span class="math inline">\(Q\)</span>.</li>
<li>The cost of moving mass from <span class="math inline">\(x\)</span> to <span class="math inline">\(T(x)\)</span> is given by <span class="math inline">\(\|x - T(x)\|^p\)</span>, raised to the power <span class="math inline">\(p\)</span>.</li>
<li>We seek the transport map <span class="math inline">\(T\)</span> that minimizes this total transport cost, denoted as <span class="math inline">\(\inf_T \|x - T(x)\|^p\)</span>.</li>
</ul>
<p>Think of this as finding the optimal way to move each grain of sand from one pile (<span class="math inline">\(P\)</span>) to another (<span class="math inline">\(Q\)</span>). The function <span class="math inline">\(T(x)\)</span> tells us where each grain should go, and we want to minimize the total effort of moving all the sand.</p>
<p>But there was a catch – sometimes this direct mapping approach proves impossible (<span class="math inline">\(T(x)\)</span> does not exist). Imagine trying to transform a single mountain (<span class="math inline">\(P\)</span>) into two smaller hills (<span class="math inline">\(Q\)</span>). How do you map one peak to two? This limitation led to a more flexible approach.</p>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up the initial and target distributions of sand piles</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>pile_1 <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">1</span>, size<span class="op">=</span><span class="dv">50</span>)  <span class="co"># Smaller sample for clarity</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>pile_2 <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">2</span>, scale<span class="op">=</span><span class="fl">1.5</span>, size<span class="op">=</span><span class="dv">50</span>)  <span class="co"># Target pile</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Transport plan: simplest one-to-one matching (sorted values for illustration)</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>pile_1_sorted <span class="op">=</span> np.sort(pile_1)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>pile_2_sorted <span class="op">=</span> np.sort(pile_2)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the multiplot</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">18</span>, <span class="dv">5</span>), sharey<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot initial pile</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>sns.histplot(pile_1, kde<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span><span class="st">"blue"</span>, ax<span class="op">=</span>axes[<span class="dv">0</span>])</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">"Initial Pile of Sand"</span>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">"Sand Position"</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">"Frequency"</span>)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot target pile</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>sns.histplot(pile_2, kde<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span><span class="st">"green"</span>, ax<span class="op">=</span>axes[<span class="dv">1</span>])</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">"Target Pile of Sand"</span>)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">"Sand Position"</span>)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot transport plan with aesthetic lines</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(pile_1_sorted)):</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">2</span>].plot([<span class="dv">0</span>, <span class="dv">1</span>], [pile_1_sorted[i], pile_2_sorted[i]], color<span class="op">=</span><span class="st">"purple"</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, lw<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].scatter([<span class="dv">0</span>]<span class="op">*</span><span class="bu">len</span>(pile_1_sorted), pile_1_sorted, color<span class="op">=</span><span class="st">"blue"</span>, label<span class="op">=</span><span class="st">"Initial Positions"</span>, zorder<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].scatter([<span class="dv">1</span>]<span class="op">*</span><span class="bu">len</span>(pile_2_sorted), pile_2_sorted, color<span class="op">=</span><span class="st">"green"</span>, label<span class="op">=</span><span class="st">"Target Positions"</span>, zorder<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_title(<span class="st">"Transport Plan"</span>)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_xticks([<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_xticklabels([<span class="st">"Initial"</span>, <span class="st">"Target"</span>])</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_xlim(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">1.5</span>)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_xlabel(<span class="st">"Sand Pile"</span>)</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_ylabel(<span class="st">"Sand Position"</span>)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].legend()</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust layout and display</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="cell-fig-KL" class="cell quarto-layout-panel page-columns page-full" data-execution_count="3" data-layout-ncol="1">
<div class="quarto-layout-row page-full">
<div class="cell-output cell-output-display quarto-layout-cell page-columns page-full" style="flex-basis: 100.0%;justify-content: flex-start;">
<div id="fig-kl" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-kl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="page-columns page-full">
<img src="index_files/figure-html/fig-kl-output-1.png" width="1712" height="464" class="figure-img column-page">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-kl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Visualization of the sandpile transport problem, including initial and target distributions as well as the transport plan.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="the-modern-solution-kantorovichs-breakthrough" class="level3" data-number="4.1.2">
<h3 data-number="4.1.2" class="anchored" data-anchor-id="the-modern-solution-kantorovichs-breakthrough"><span class="header-section-number">4.1.2</span> The Modern Solution: Kantorovich’s Breakthrough</h3>
<p>Leonid Kantorovich revolutionized the field by allowing us to split and combine masses. His formulation looks like this:</p>
<p><span class="math display">\[
W_p(P, Q) = \left( \inf_{J \in \mathcal{J}(P, Q)} \int_{\mathbb{R}^d \times \mathbb{R}^d} \|x - y\|^p \, dJ(x, y) \right)^{1/p}
\]</span></p>
<ul>
<li><p><strong>J(x, y)</strong>: A joint distribution over (x, y) pairs. Think of J as specifying how much “mass” at x under P is transported to y under Q.</p></li>
<li><p><strong>J(P, Q)</strong>: The set of all possible joint distributions J whose marginals are P and Q.</p></li>
<li><p>Marginal Condition :</p>
<ul>
<li><span class="math inline">\(\int J(x, y) \, dy = P(x)\)</span>, All mass at x under P is accounted for.<br>
</li>
<li><span class="math inline">\(\int J(x, y) \, dx = Q(y)\)</span>, All mass at y under Q is accounted for.</li>
</ul></li>
<li><p><span class="math inline">\(∥x - y∥^p\)</span> : The cost of transporting mass between x and y.</p></li>
</ul>
<p>Instead of finding a direct map, we now look for a “transport plan” <span class="math inline">\(J(x,y)\)</span> that tells us how much mass to move from each point <span class="math inline">\(x\)</span> to each point <span class="math inline">\(y\)</span>. It’s like having a shipping manifest that details how much sand moves between any two locations.</p>
<p>An interesting point is the fact that the Kantorovich problems can be express a dual formula.</p>
<p>In this dual formulation, instead of finding a transport plan <strong>J</strong>, we find functions <span class="math inline">\(\phi(x)\)</span> and <span class="math inline">\(\psi(y)\)</span> that describe the “potential energy” of moving mass from <strong>P</strong> to <strong>Q</strong>. The supremum is taken over all measurable functions <span class="math inline">\(\psi\)</span> and <span class="math inline">\(\phi\)</span>.</p>
<p><span class="math display">\[\mathcal{W}_p(P, Q) = \sup_{\psi, \phi} \left( \int_{R^ d}\psi(y)\,dQ(y) - \int_{R^d}\phi(x)\,dP(x) \right)\]</span></p>
<p>where <span class="math inline">\(\psi(y) - \phi(x) \leq \|x - y\|^p\)</span></p>
<p>So, instead of finding a transport plan J, we find functions <span class="math inline">\(\phi(x)\)</span> and <span class="math inline">\(\psi(y)\)</span> that describe the “potential energy” of moving mass from distribution P to Q. These functions, often referred to as potentials or couplings, quantify the minimal cumulative cost required to transport mass from any point x in the source (P) to a point with zero potential energy in the target (Q), and vice versa. By optimizing over these potentials, we can efficiently compute the minimal cost of transporting all mass from P to Q.</p>
<p>But before seeing what does thies mean let’s look some special case.</p>
</section>
</section>
<section id="special-cases-and-practical-trick" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="special-cases-and-practical-trick"><span class="header-section-number">4.2</span> Special Cases and Practical Trick</h2>
<section id="the-one-dimensional-miracle" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="the-one-dimensional-miracle"><span class="header-section-number">4.2.1</span> The One-Dimensional Miracle</h3>
<p>In one dimension, something magical happens. The Wasserstein distance becomes surprisingly simple to compute:</p>
<p><span class="math display">\[
W_p(P, Q) = \left( \int_{0}^1 \left| F^{-1}(z) - G^{-1}(z) \right|^p dz \right)^{1/p}
\]</span></p>
<p>Become for <span class="math inline">\(p=2\)</span> the simple : This is like comparing two distributions by aligning their quantiles and measuring the average distance between corresponding points. It’s particularly useful when working with simple univariate distributions.</p>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">8</span>, <span class="dv">200</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>dist1 <span class="op">=</span> stats.norm.pdf(x, loc<span class="op">=</span><span class="dv">3</span>, scale<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>dist2 <span class="op">=</span> stats.norm.pdf(x, loc<span class="op">=</span><span class="dv">4</span>, scale<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create plot</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>plt.plot(x, dist1, label<span class="op">=</span><span class="st">'Distribution P'</span>, color<span class="op">=</span><span class="st">'#8884d8'</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>plt.plot(x, dist2, label<span class="op">=</span><span class="st">'Distribution Q'</span>, color<span class="op">=</span><span class="st">'#82ca9d'</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Add quantile lines</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>quantile_points <span class="op">=</span> [<span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>]</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> q <span class="kw">in</span> quantile_points:</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    q1 <span class="op">=</span> stats.norm.ppf(q, loc<span class="op">=</span><span class="dv">3</span>, scale<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    q2 <span class="op">=</span> stats.norm.ppf(q, loc<span class="op">=</span><span class="dv">4</span>, scale<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    plt.vlines(q1, <span class="dv">0</span>, stats.norm.pdf(q1, loc<span class="op">=</span><span class="dv">3</span>, scale<span class="op">=</span><span class="fl">0.5</span>), </span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>               colors<span class="op">=</span><span class="st">'#8884d8'</span>, linestyles<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    plt.vlines(q2, <span class="dv">0</span>, stats.norm.pdf(q2, loc<span class="op">=</span><span class="dv">4</span>, scale<span class="op">=</span><span class="fl">0.7</span>), </span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>               colors<span class="op">=</span><span class="st">'#82ca9d'</span>, linestyles<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    plt.plot([q1, q2], </span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>            [stats.norm.pdf(q1, loc<span class="op">=</span><span class="dv">3</span>, scale<span class="op">=</span><span class="fl">0.5</span>), </span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>             stats.norm.pdf(q2, loc<span class="op">=</span><span class="dv">4</span>, scale<span class="op">=</span><span class="fl">0.7</span>)], </span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>            <span class="st">'gray'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Wasserstein Distance Visualization'</span>)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="cell-fig-was" class="cell quarto-layout-panel" data-execution_count="4" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="cell-output cell-output-display quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div id="fig-was" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-was-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-was-output-1.png" width="816" height="529" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-was-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Visualization of the Wasserstein Distance between two normal distributions with annotated quantile lines.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="the-gaussian-case-a-beautiful-formula" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="the-gaussian-case-a-beautiful-formula"><span class="header-section-number">4.2.2</span> The Gaussian Case: A Beautiful Formula</h3>
<p>When comparing two Gaussian distributions, we get an especially elegant result:</p>
<p><span class="math display">\[
W_2^2(P, Q) = \|\mu_1 - \mu_2\|^2 + \text{trace}(\Sigma_1 + \Sigma_2 - 2(\Sigma_1^{1/2} \Sigma_2 \Sigma_1^{1/2})^{1/2})
\]</span></p>
<ul>
<li><strong>Distance between the means:</strong> <span class="math inline">\(\|\mu_1 - \mu_2\|_2\)</span> Measures the Euclidean distance between the means of two distributions.</li>
<li><strong>Trace:</strong> <span class="math inline">\(\text{Tr}(\Sigma_1 - \Sigma_2)^2\)</span> Measures differences in the covariance structures, where <span class="math inline">\(\Sigma_1\)</span> and <span class="math inline">\(\Sigma_2\)</span> are the covariance matrices of the respective distributions.</li>
</ul>
<p>This formula neatly separates the difference between means from the difference in spread and shape.</p>
</section>
</section>
<section id="making-it-practical-the-sinkhorn-algorithm" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="making-it-practical-the-sinkhorn-algorithm"><span class="header-section-number">4.3</span> Making It Practical: The Sinkhorn Algorithm</h2>
<p>In real-world applications, when <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> are discrete distribution, computing the exact Wasserstein distance can be computationally intensive. Enter the Sinkhorn algorithm, which adds a touch of entropy regularization:</p>
<p><span class="math display">\[
W_p^\lambda(P, Q) = \inf_{J \in \mathcal{J}(P, Q)} \int_{\mathbb{R}^d \times \mathbb{R}^d} \|x - y\|^p dJ(x, y) + \lambda \int_{\mathbb{R}^d \times \mathbb{R}^d} J(x, y) \log J(x, y)
\]</span></p>
<p>This regularization makes the computation much more tractable while still preserving the essential properties of the Wasserstein distance.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Mistral Nemo Explain 👨🏻‍🏫">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mistral Nemo Explain 👨🏻‍🏫
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><p><strong>Entropy Regularization</strong>: The key idea behind entropy regularization is to add a term proportional to the negative entropy (or KL-divergence) of the transport plan <strong>J</strong> to the original optimization problem. This has two main effects:</p>
<ul>
<li>It encourages <strong>J</strong> to be more “uniform” or “entropic,” making it easier to optimize.</li>
<li>It introduces a bias towards transports that are close to the uniform distribution, making the problem smoother and better suited for numerical methods like the Sinkhorn algorithm.</li>
</ul></li>
<li><p><strong>Sinkhorn Iterations</strong>: The Sinkhorn algorithm is an iterative method that finds an approximate solution to the entropy-regularized Wasserstein distance minimization problem. It works by alternating between updating two potential functions, <span class="math inline">\(\phi(x)\)</span> (for source distribution <strong>P</strong>) and <span class="math inline">\(\psi(y)\)</span> (for target distribution <strong>Q</strong>), while keeping track of a transport plan <strong>K</strong> that is row-wise and column-wise stochastic (i.e., each row and column sums to 1). Here’s how the algorithm progresses:</p>
<ul>
<li><p>Initialize potential functions <span class="math inline">\(\phi^{(0)}(x) = \log P(x)\)</span> and <span class="math inline">\(\psi^{(0)}(y) = \log Q(y)\)</span>, or some initial guess.</p></li>
<li><p>Iterate the following steps until convergence:</p>
<ol type="1">
<li><p><strong>Row Update</strong>: Compute a new transport plan <strong>K</strong> by normalizing the row-wise product of potentials: <span class="math display">\[
K_{ij} \propto P_i Q_j e^{-\|x_i - y_j\|_p / \lambda} e^{\phi^{(t)}(x_i) + \psi^{(t)}(y_j)}
\]</span> where <span class="math inline">\(P_i\)</span> and <span class="math inline">\(Q_j\)</span> are the discrete probabilities for sources and targets, respectively.</p></li>
<li><p><strong>Column Update</strong>: Normalize the columns of <strong>K</strong> to obtain an updated transport plan <strong>K’</strong>, and then compute new potentials: <span class="math display">\[
\phi^{(t+1)}(x_i) = \log \left( \sum_j K'_{ij} e^{- \|x_i - y_j\|_p / \lambda} e^{\psi^{(t)}(y_j)} \right)
\]</span> <span class="math display">\[
\psi^{(t+1)}(y_j) = \phi^{(t+1)}(x_0) + \frac{\|x_0 - y_j\|_p}{\lambda} - \log Q_j
\]</span> where <span class="math inline">\(x_0\)</span> is an arbitrary point (often chosen as the mean or median of <strong>P</strong>).</p></li>
</ol></li>
</ul></li>
<li><p><strong>Convergence</strong>: The Sinkhorn algorithm is guaranteed to converge for any initial potentials, and the limit transport plan <strong>K</strong> satisfies a fixed-point equation that corresponds to the solution of the entropy-regularized Wasserstein distance minimization problem. Moreover, as <span class="math inline">\(\lambda \to 0\)</span>, the approximate solution converges to the exact solution of the original (unregularized) problem.</p></li>
<li><p><strong>Advantages</strong>: The main advantages of using the Sinkhorn algorithm with entropy regularization are:</p>
<ul>
<li>It provides an efficient and easy-to-implement method for computing approximations of the Wasserstein distance between discrete distributions.</li>
<li>It can handle large-scale problems, as it only requires solving a sequence of linear systems at each iteration.</li>
<li>It is robust to initialization and converges quickly in practice.</li>
</ul></li>
</ol>
</div>
</div>
</div>
<p><strong>To resume</strong>: Given two distributions <strong>P</strong> and <strong>Q</strong>, the core idea of the Wasserstein distance is to find the “least-cost” way to transport mass from <strong>P</strong> to <strong>Q</strong>. This problem can be formulated either as a Monge map (direct transportation) or a Kantorovich potential (joint distribution).<br>
The output is a scalar distance <span class="math inline">\(\mathcal{W}_p(\textbf{P}, \textbf{Q})\)</span> that quantifies the minimal cumulative cost of transporting all mass from <strong>P</strong> to <strong>Q</strong>, where costs are measured using the <span class="math inline">\(L_p\)</span> norm. Optionally, an explicit transport plan or map can also be provided.</p>
<p>This formulation enables us to compare and analyze distributions based on their relative transportation costs, making it a powerful tool in various applications such as machine learning, shape analysis, and optimal transport theory. In the next part, we will explore practical algorithms for computing the Wasserstein distance between discrete distributions.</p>
</section>
</section>
<section id="problem-solving-how-to-find-the-optimal-path-for-1d-distribution" class="level1 page-columns page-full" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Problem Solving : How to find the optimal path for 1D distribution ?</h1>
<p>Now let’s focusing on explaining the optimal transport method for 1D distributions and its relation to Wasserstein distance. Yes, it is time for an an easy practical example.</p>
<section id="optimal-transport-for-1d-distributions-finding-the-best-path" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="optimal-transport-for-1d-distributions-finding-the-best-path"><span class="header-section-number">5.1</span> Optimal Transport for 1D Distributions: Finding the Best Path</h2>
<p>In this section, we explore how to compute the <strong>“optimal transport plan”</strong> between two 1D distributions <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(q(x)\)</span>.<br>
As already explain, the concept of optimal transport lies at the heart of the Wasserstein distance, which measures the “effort” required to morph one distribution into another.</p>
<p>This is particularly useful for comparing probability distributions, whether as a loss function in machine learning or for creating monitoring metrics.</p>
<p>Let’s explain what will happen in the code.</p>
</section>
<section id="first-stage-the-cost-matrix" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="first-stage-the-cost-matrix"><span class="header-section-number">5.2</span> First Stage: The Cost Matrix</h2>
<p>To determine the optimal transport plan, we first define a <strong>cost matrix</strong> <span class="math inline">\(C\)</span>. Each entry <span class="math inline">\(C[i][j]\)</span> represents the cost of moving “mass” from bin <span class="math inline">\(i\)</span> of <span class="math inline">\(p(x)\)</span> to bin <span class="math inline">\(j\)</span> of <span class="math inline">\(q(x)\)</span>. For this example, we use the squared Euclidean distance between bins:</p>
<p><span class="math display">\[
C[i][j] = (i - j)^2
\]</span></p>
<p>This choice ensures that movements over longer distances incur a higher cost, reflecting their physical or conceptual separation.</p>
</section>
<section id="second-stage-linear-programming-approach-for-finding-the-best-transport-plan." class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="second-stage-linear-programming-approach-for-finding-the-best-transport-plan."><span class="header-section-number">5.3</span> Second Stage : Linear Programming Approach for finding the best Transport Plan.</h2>
<p>The optimal transport problem can be formulated as a linear programming (LP) problem. Our goal is to find a transport plan <span class="math inline">\(T\)</span>, where <span class="math inline">\(T[i][j]\)</span> specifies the amount of mass to move from bin <span class="math inline">\(i\)</span> of <span class="math inline">\(p(x)\)</span> to bin <span class="math inline">\(j\)</span> of <span class="math inline">\(q(x)\)</span>, such that:</p>
<ol type="1">
<li><p>The <strong>row sums</strong> of <span class="math inline">\(T\)</span> equal <span class="math inline">\(p\)</span>, ensuring all mass in <span class="math inline">\(p(x)\)</span> is transported: <span class="math display">\[
\sum_{j} T[i][j] = p[i] \quad \forall i
\]</span></p></li>
<li><p>The <strong>column sums</strong> of <span class="math inline">\(T\)</span> equal <span class="math inline">\(q\)</span>, ensuring all mass in <span class="math inline">\(q(x)\)</span> is received: <span class="math display">\[
\sum_{i} T[i][j] = q[j] \quad \forall j
\]</span></p></li>
<li><p>The total cost is minimized: <span class="math display">\[
\text{Minimize } \sum_{i, j} T[i][j] \cdot C[i][j]
\]</span></p></li>
</ol>
<p>We use the <code>scipy.optimize.linprog</code> function to solve this LP problem. The result is a transport plan <span class="math inline">\(T\)</span> that minimizes the total cost while satisfying the marginal constraints.</p>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Personal introspection
</div>
</div>
<div class="callout-body-container callout-body">
<p>As I write this, I’m thinking back to some of my courses on path-finding algorithms like A* and Dijkstra. Maybe there’s a link?</p>
</div>
</div>
</section>
<section id="third-code-walkthrough" class="level2 page-columns page-full" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="third-code-walkthrough"><span class="header-section-number">5.4</span> Third : Code Walkthrough</h2>
<p>Let’s resume what we want to observe:</p>
<ol type="1">
<li><p><strong>The Density Functions</strong><br>
The kernel density estimates (KDEs) of <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(q(x)\)</span> give an intuitive view of the distributions.</p></li>
<li><p><strong>The Cost Matrix</strong><br>
The heatmap of the cost matrix <span class="math inline">\(C\)</span> illustrates the squared distances between bins. Higher values indicate a higher cost of transportation.</p></li>
<li><p><strong>The Optimal Transport Plan</strong><br>
By visualized <span class="math inline">\(T\)</span> as a heatmap we will shows the amounts of mass transported from each bin of <span class="math inline">\(p(x)\)</span> to <span class="math inline">\(q(x)\)</span>.</p></li>
</ol>
<p>Below are the results of applying the above concepts:</p>
<div id="eec98ad3" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> linprog</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>nb_b <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> np.random.rand(nb_b)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> p <span class="op">/</span> p.<span class="bu">sum</span>()  </span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> np.random.rand(nb_b)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> q <span class="op">/</span> q.<span class="bu">sum</span>()  </span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the cost matrix C based on squared Euclidean distance</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>bins <span class="op">=</span> <span class="bu">len</span>(p)  </span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> np.square(np.subtract.outer(np.arange(bins), np.arange(bins)))  <span class="co"># Squared distance</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Flatten distributions and cost matrix for linear programming</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> C.flatten()  <span class="co"># Cost vector (flattened cost matrix)</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>A_eq <span class="op">=</span> np.zeros((<span class="dv">2</span> <span class="op">*</span> bins, bins <span class="op">*</span> bins))  <span class="co"># Equality constraints matrix</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>b_eq <span class="op">=</span> np.concatenate([p, q])  <span class="co"># Marginal sums for p and q</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Build the A_eq matrix to enforce the marginals for p and q</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(bins):</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    A_eq[i, i <span class="op">*</span> bins: (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> bins] <span class="op">=</span> <span class="dv">1</span>  <span class="co"># For row sums to p[i]</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    A_eq[i <span class="op">+</span> bins, i::bins] <span class="op">=</span> <span class="dv">1</span>  <span class="co"># For column sums to q[i]</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Solve the linear programming problem</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> linprog(c, A_eq<span class="op">=</span>A_eq, b_eq<span class="op">=</span>b_eq, method<span class="op">=</span><span class="st">'highs'</span>)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Retrieve the transport plan matrix T from the solution</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> result.x.reshape((bins, bins))</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_density(p, q):</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    _, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">5</span>))</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>    sns.kdeplot(p, color<span class="op">=</span><span class="st">'blue'</span>, ax<span class="op">=</span>ax1, linewidth<span class="op">=</span><span class="dv">2</span>, bw_adjust<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>    ax1.set_title(<span class="st">'p(x)'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>    ax1.set_xlabel(<span class="st">'Bins'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>    ax1.set_ylabel(<span class="st">'Density'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>    sns.kdeplot(q, color<span class="op">=</span><span class="st">'orange'</span>, ax<span class="op">=</span>ax2, linewidth<span class="op">=</span><span class="dv">2</span>, bw_adjust<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>    ax2.set_title(<span class="st">'q(x)'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>    ax2.set_xlabel(<span class="st">'Bins'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>    ax2.set_ylabel(<span class="st">'Density'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_cost_matrix(C, bins):</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>,<span class="dv">5</span>))</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>    sns.heatmap(C, annot<span class="op">=</span><span class="va">False</span>, fmt<span class="op">=</span><span class="st">'.1f'</span>, cmap<span class="op">=</span>sns.color_palette(<span class="st">"light:blue"</span>, as_cmap<span class="op">=</span><span class="va">True</span>), cbar<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>                xticklabels<span class="op">=</span>[<span class="ss">f'</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(bins)],</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>                yticklabels<span class="op">=</span>[<span class="ss">f'</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(bins)],</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>                cbar_kws<span class="op">=</span>{<span class="st">'label'</span>: <span class="st">'Squared Distance'</span>})</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Cost Matrix: Squared Euclidean Distance'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Bins of Distribution q'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Bins of Distribution p'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_transport_plan(T, bins):</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>,<span class="dv">5</span>))</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>    sns.heatmap(T, annot<span class="op">=</span><span class="va">False</span>, fmt<span class="op">=</span><span class="st">'.2f'</span>, cmap<span class="op">=</span>sns.color_palette(<span class="st">"light:blue"</span>, as_cmap<span class="op">=</span><span class="va">True</span>), cbar<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a>                xticklabels<span class="op">=</span>[<span class="ss">f'</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(bins)],</span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a>                yticklabels<span class="op">=</span>[<span class="ss">f'</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(bins)],</span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>                cbar_kws<span class="op">=</span>{<span class="st">'label'</span>: <span class="st">'Transport Amount'</span>})</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">"Optimal Transport Plan"</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"Bins of Distribution q"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"Bins of Distribution p"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>plot_density(p, q)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>plot_cost_matrix(C, bins)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="cell-fig-cost" class="cell column-page quarto-layout-panel" data-execution_count="6" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="cell-output cell-output-display quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-cost" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cost-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-cost-output-1.png" width="463" height="463" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cost-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Density plots of the two distributions p and q
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-cost" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cost-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-cost-output-2.png" width="465" height="463" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cost-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Cost matrix based on squared Euclidean distance between bins of distributions p and q.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>plot_density(p, q)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>plot_transport_plan(T, bins)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="cell-fig-transport" class="cell column-page quarto-layout-panel" data-execution_count="7" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="cell-output cell-output-display quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-transport" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transport-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-transport-output-1.png" width="463" height="463" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transport-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Density plots of the two distributions p and q
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-transport" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transport-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-transport-output-2.png" width="465" height="463" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transport-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Optimal transport plan showing the transport amounts between bins of p and q.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>The visualization of the optimal transport plan clearly shows how <span class="math inline">\(p(x)\)</span> is “redistributed” to align with <span class="math inline">\(q(x)\)</span>.<br>
Each nonzero element of <code>T</code> corresponds to a connection between <span class="math inline">\(p[i]\)</span> and <span class="math inline">\(q[j]\)</span>, with the intensity reflecting the amount of mass transported.</p>
<p>But what’s the connection with the Wasserstein Distance ?</p>
<p>Her, the optimal transport plan <span class="math inline">\(T\)</span> allows us to compute the <strong>Wasserstein distance</strong> as the square root of the total transport cost:</p>
<p><span class="math display">\[
W_2(p, q) = \sqrt{\sum_{i, j} T[i][j] \cdot C[i][j]}
\]</span></p>
<p>This metric provides a robust way to compare distributions, considering both the location and “effort” to match their mass. It is particularly advantageous over other metrics like Kullback-Leibler divergence for distributions with non-overlapping supports.</p>
<p>Of course, it is a simple and non optimal exemple, and algorythm like Sinkhorn algorithm will work better.</p>
</section>
</section>
<section id="this-is-the-end" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> This is the end</h1>
<p>Understanding distances between distributions is a fundamental challenge in data science. By framing the problem in terms of optimal transport – moving piles of sand from one distribution to another – it provides an intuitive and mathematically robust way to compare distributions.</p>
<p>As machine learning continues to evolve, tools like the Wasserstein distance become increasingly important. Whether you’re evaluating GANs, WVAE, performing domain adaptation, or monitoring model drift, understanding how to measure and interpret distances between distributions is crucial. While the mathematics may seem daunting at first, the underlying intuition of optimal transport – finding the most efficient way to move probability mass – provides a concrete framework for thinking about these abstract concepts.</p>
<p>Keep in mind that while we’ve focused on some specific implementations and special cases, there’s much more to explore. The field of optimal transport continues to develop, with new algorithms and applications emerging regularly.</p>
<p>Remember: Sometimes the best way to understand how things are different is to figure out what it takes to make them the same – and that’s exactly what the Wasserstein distance helps us quantify</p>
<p>Thanks for your time reading my post !</p>
</section>
<section id="references" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> References</h1>
<ol type="1">
<li><a href="https://people.csail.mit.edu/jsolomon/assets/optimal_transport.pdf?utm_source=chatgpt.com">“Optimal Transport on Discrete Domains” by Justin Solomon</a>: This paper explores optimal transport theory in the context of discrete domains, providing insights into computational methods and applications.</li>
<li><a href="https://alexhwilliams.info/itsneuronalblog/2020/10/09/optimal-transport/?utm_source=chatgpt.com">“A Short Introduction to Optimal Transport and Wasserstein Distance” by Alex H. Williams</a>: This blog post offers an intuitive overview of optimal transport theory and Wasserstein distance.</li>
<li><a href="https://www.stat.cmu.edu/~larry/%3Dsml/Opt.pdf?utm_source=chatgpt.com">“Optimal Transport and Wasserstein Distance” by Larry Wasserman</a>: This PDF document delves into the mathematical foundations of optimal transport and Wasserstein distance, exploring their applications in statistics and machine learning.</li>
<li><a href="https://arxiv.org/abs/1709.10219?utm_source=chatgpt.com">“Information Geometry Connecting Wasserstein Distance and Kullback-Leibler Divergence via the Entropy-Relaxed Transportation Problem” by Shun-ichi Amari et al.</a>: This research paper presents a unified framework linking Wasserstein distance and KL divergence through entropy-relaxed optimal transport ; Deeper Math.</li>
<li><a href="https://calvinmccarter.wordpress.com/2022/03/29/mapping-between-two-gaussians-using-optimal-transport-and-the-kl-divergence/?utm_source=chatgpt.com">“Mapping Between Two Gaussians Using Optimal Transport and the KL Divergence” by Calvin McCarter</a>: This post explores the relationship between optimal transport and KL divergence</li>
<li><a href="&quot;https://lilianweng.github.io/posts/2017-08-20-gan/&quot;">“From GAN to WGAN” by Lilian Weng</a>: This comprehensive post delves into Generative Adversarial Networks (GANs) and introduces the concept of Wasserstein GANs (WGANs).</li>
<li><a href="https://www.stat.cmu.edu/~cshalizi/754/2006/notes/lecture-28.pdf">Shannon Entropy and Kullback-Leibler Divergence</a></li>
</ol>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{hayer2024,
  author = {Hayer, Tancrede},
  title = {The {Quest} for Understanding {Distribution} {Distance:}
    {From} {Theory} to {Practice}},
  date = {2024-12-27},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-hayer2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Hayer, Tancrede. 2024. <span>“The Quest for Understanding Distribution
Distance: From Theory to Practice.”</span> December 27, 2024.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="gituser/gitrepo" data-repo-id="" data-category="General" data-category-id="" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->




</body></html>
{
  "hash": "65b8f644643dac6487cfacce016193f0",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"The Quest for understanding Distribution Distance: From Theory to Practice\"\nsubtitle: \"Understanding the Mathematics (but not too many) of Distribution Comparison\"\nauthor: \n    - name: \"Tancrede Hayer\"\ndate: 12-27-2024\ncategories: [Information Theory, Math]\nnumber-sections: true\ntitle-block-banner: \"#f0f3f5\"\ntitle-block-banner-color: \"black\"\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: true\nexecute:\n  freeze: auto\n  cache: true\nimage: preview-image.png\ncitation: true\n---\n\nHave you ever wondered how we measure how \"far apart\" two distributions are in data science? It's not as simple as it might seem, but don't worry – we'll keep the math tame and focus on intuition.\n\nBy the end of this reading, you will have a better understanding of optimal transport theory and how it helps us in data science and model monitoring.\n\n# Why do we care about piles of sand ?\n\nAn important topic in machine learning (and math in general) is to find a useful way to measure \"distance\" between pairs of distributions.\n\nAmong their properties, symmetry and the triangle inequality play central roles in ensuring that these measures behave intuitively and are mathematically robust.\n\n:::{.callout-note title=\"Why ?\" collapse=\"false\"}\nSymmetry is distance metrics between two distributions `P` and `Q` ensures their distance remains unchanged regardless of which distribution is considered first $d(P, Q) = d(Q, P)$. \n\nThis symmetry allows for a fair and consistent comparison, as the direction of measurement doesn't affect the outcome.\n\nThe triangle inequality property guarantees that the distance between any two points (distributions) is always less than or equal to the sum of the distances to a third point. For three distributions `P`, `Q`, and `R`, this is expressed as $d(P, R) ≤ d(P, Q) + d(Q, R)$.  \n\nThis  property ensures that our distance metrics align with the intuitive notion of \"shortest path.\"\n:::\n\n\n# The Kullback-Leibler and Jensen-Shannon divergence\n## The Kullback-Leibler divergence\nImagine you're in a bustling city, and you have two maps, $P$ and $Q$, trying to guide you through the streets. Now, we want to know which map is more reliable when navigating from point A to B (by comparing an outdated map vs a new official map). Here comes KL divergence, to help us out!\n\nThe KL Divergence measures how one probability distribution $Q$ diverges from another, $P$ :\n\n$$\nD_{KL}(P \\| Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}\n$$\n\nFor continuous distributions it is :\n\n$$\nD_{KL}(P \\| Q) = \\int_{-\\infty}^{\\infty} p(x) \\log \\frac{p(x)}{q(x)} dx\n$$\n\nIn this cityscape analogy, the KL divergence measures how much extra walking you'd do if you used map $Q$ instead of the true map $P$. When following directions from $Q$, the additional distance traveled is proportional to $D_{KL}(P \\| Q)$. It's like a penalty for when $Q$ underestimates or misrepresents the probability of going through certain streets compared to $P$.\n\n:::{.callout-tip title=\"Concrete cityscape exemple\" collapse=\"true\"}\nLet's make this concrete with a simple city layout having three key locations: Home (H), Work (W), and Shopping Mall (S).\n\nThe true map $P$ shows these probabilities of successful routes:\n\n- $P(H→W) = 0.5$ (a direct highway route)\n- $P(H→S) = 0.3$ (a clear urban route)\n- $P(H→M) = 0.2$ (a residential path)\n\nWhile the approximate map $Q$ shows:\n\n- $Q(H→W) = 0.4$ (underestimates highway access)\n- $Q(H→S) = 0.4$ (overestimates this route)\n- $Q(H→M) = 0.2$ (correctly estimates this one)\n\nLet's calculate this for our city example:\n\n- For the Work route:\n$$P(H→W) * log(P(H→W)/Q(H→W)) = 0.5 * log(0.5/0.4) ≈ 0.048$$\n- For the Shopping route:\n$$P(H→S) * log(P(H→S)/Q(H→S)) = 0.3 * log(0.3/0.4) ≈ -0.037$$\n- For the Mall route:\n$$P(H→M) * log(P(H→M)/Q(H→M)) = 0.2 * log(0.2/0.2) ≈ 0$$\n\nTotal KL divergence = 0.048  -0.037 + 0 = 0.011\n\nThis means using map Q instead of P results in about 1.1% inefficiency in navigation. In other words, you'll spend about 1.1% more time or distance traveling when using the approximate map Q compared to the true map P.\n:::\n\nThe KL divergence quantifies the inefficiency of assuming that the distribution $Q$ approximates the true distribution $P$. When encoding data points sampled from $P$ using a code optimized for $Q$, the additional encoding length is proportional to $D_{KL}(P \\| Q)$.  \n\nNow consider, for example, the log term $\\log \\frac{P(x)}{Q(x)}$. If $P$ suggests you go down an alley while $Q$ shows it's a dead end (i.e., $Q(x)$ approaches zero but $P(x)$ doesn't), that's when our navigator (KL divergence) really starts warning you, \"Watch out! You're about to take a long detour!\"\n\nThe log term, $\\log \\frac{P(x)}{Q(x)}$, doesn't just warn you - it declares the entire route using Q as invalid (by giving it an infinite penalty) for underestimating probabilities in $Q$ compared to $P$.\n\nKL divergence heavily penalizes cases where $Q(x)$ approaches zero while $P(x)$ remains non-zero.\n\nBefore continuing let's keep in mind the special properties of KL divergence through this lens:\n\n1. **Non-Negativity**: $D_{KL}(P \\| Q) \\geq 0$ - Using either map will never lead you astray; at worst, they'll guide you equally well (when $P = Q$).\n\n2. **Zero Minimum**: $D_{KL}(P \\| Q) = 0 \\iff P = Q$ - The only way our navigator won't make you walk extra is if both maps are the same.\n\n3. **Asymmetric**: $D_{KL}(P \\| Q) \\neq D_{KL}(Q \\| P)$ - Our navigator isn't biased ; he'll give different walking distance estimates depending on which map you're using as your reference.\n\n\n## Jensen–Shannon Divergence\n\nContinuing our city analogy, imagine you have two different navigation systems (representing distributions P and Q) giving you directions from point A to B. Using KL divergence, you could measure how much extra distance you'd walk if you followed Q's directions when P actually knows the correct route (KL(P||Q)), or vice versa (KL(Q||P)). But what if, instead of just measuring how wrong one system is compared to the other, you want to understand how different these two navigation systems are from each other? Here's where the Jensen-Shannon (JS) divergence comes in.\n\nWe can see the JS Divergence as a symmetrized and smoother version of the KL Divergence, defined as:\n\n$$\nD_{JS}(P \\| Q) = \\frac{1}{2} D_{KL}(P \\| M) + \\frac{1}{2} D_{KL}(Q \\| M), \\text{ where } M = \\frac{1}{2}(P + Q)\n$$\n\n:::{.callout-tip title=\"Concreate example\" collapse=\"true\"}\n**Detailed Calculation of Jensen-Shannon Divergence**\n\nLet's work through a detailed calculation using our city navigation example to better understand how JS divergence works in practice.\n\n- Step 1: Calculate M\n\nGiven our probability distributions:\nP = [0.5, 0.3, 0.2] (true map)\nQ = [0.4, 0.4, 0.2] (approximate map)\n\nThe average distribution M is calculated as:\n\n$$\nM = \\frac{1}{2}(P + Q) = \\left[\\frac{0.5 + 0.4}{2}, \\frac{0.3 + 0.4}{2}, \\frac{0.2 + 0.2}{2}\\right] = [0.45, 0.35, 0.2]\n$$\n\n- Step 2: Calculate D_KL(P||M)\n\nThe KL divergence from M to P is calculated as:\n\n$$\nD_{KL}(P \\parallel M) = \\sum_{i} P(i) \\cdot \\log\\left(\\frac{P(i)}{M(i)}\\right)\n$$\n\nLet's calculate each term:\n\n1. For H→W route:\n$$\nP(0.5) \\cdot \\log\\left(\\frac{0.5}{0.45}\\right) = 0.5 \\cdot \\log(1.1111) = 0.5 \\cdot 0.0458 = 0.0229\n$$\n\n2. For H→S route:\n$$\nP(0.3) \\cdot \\log\\left(\\frac{0.3}{0.35}\\right) = 0.3 \\cdot \\log(0.8571) = 0.3 \\cdot (-0.0669) = -0.0201\n$$\n\n3. For H→M route:\n$$\nP(0.2) \\cdot \\log\\left(\\frac{0.2}{0.2}\\right) = 0.2 \\cdot \\log(1) = 0\n$$\n\n4. Sum\n$$ \nD_{KL}(P \\parallel M) = 0.0229 - 0.0201 + 0 = 0.0028\n$$\n\n- Step 3: Calculate D_KL(Q||M)\n\nSimilarly for Q:\n\n$$\nD_{KL}(Q \\parallel M) = \\sum_{i} Q(i) \\cdot \\log\\left(\\frac{Q(i)}{M(i)}\\right)\n$$\n\n1. For H→W route:\n$$\nQ(0.4) \\cdot \\log\\left(\\frac{0.4}{0.45}\\right) = 0.4 \\cdot \\log(0.8889) = 0.4 \\cdot (-0.0493) = -0.0197\n$$\n\n1. For H→S route:\n$$\nQ(0.4) \\cdot \\log\\left(\\frac{0.4}{0.35}\\right) = 0.4 \\cdot \\log(1.1429) = 0.4 \\cdot 0.0571 = 0.0228\n$$\n\n1. For H→M route:\n$$\nQ(0.2) \\cdot \\log\\left(\\frac{0.2}{0.2}\\right) = 0.2 \\cdot \\log(1) = 0\n$$\n\n1. Sum: \n$$\nD_{KL}(Q \\parallel M) = -0.0197 + 0.0228 + 0 = 0.0031\n$$ \n\n- Step 4: Calculate D_JS(P||Q)\n\nThe final JS divergence is calculated as:\n\n$$\nD_{JS}(P \\parallel Q) = \\frac{1}{2} D_{KL}(P \\parallel M) + \\frac{1}{2} D_{KL}(Q \\parallel M)\n$$\n\nSubstituting our values:\n\n$$\nD_{JS}(P \\parallel Q) = \\frac{1}{2}(0.0028) + \\frac{1}{2}(0.0031) = 0.0014 + 0.00155 = 0.00295\n$$\n\nThe small JS divergence value (0.00295) indicates that our two maps P and Q are fairly similar in their routing suggestions. This makes sense because:\n\n1. They completely agree on the probability of the residential path (H→M: 0.2)\n2. They only slightly disagree on the highway route (0.5 vs 0.4)\n3. Their difference in the urban route (0.3 vs 0.4) is moderate\n\nThis numerical example shows how JS divergence provides a symmetric and bounded measure of the difference between two probability distributions, making it particularly useful for comparing different routing strategies or maps.\n:::\n\nThe JS divergence finds a middle ground between maps $P$ and $Q$ by averaging them into a new map $M = \\frac{1}{2}(P + Q)$. It then measures how different this average route is from each original one, giving equal weight to both.\n\nBy combining and averaging the maps, our navigator (JS divergence) ensures a more stable and symmetric measure of their differences. This interpolation also helps when one map suggests going through a street that the other doesn't – instead of getting stuck, our navigator finds a sensible alternative route.\n\nUnlike KL divergence, the JS divergence measures the similarity of two distributions by incorporating their average, $M$. By interpolating between $P$ and $Q$, JS divergence ensures a bounded and symmetric measure. This interpolation resolves issues of infinite divergence where $P(x) > 0$ but $Q(x) = 0$.\n\nAnd let's revisit the special properties of JS divergence using this city analogy:\n\n1. **Symmetric**: $D_{JS}(P \\| Q) = D_{JS}(Q \\| P)$ - Our compromising navigator gives equal importance to both maps and provides a balanced measure of their differences, ensuring you're not favoring one over the other.\n\n2. **Bounded**: $0 \\leq D_{JS}(P \\| Q) \\leq 1$ - The navigator's score will always fall within a reasonable range, making it easier for you to gauge how similar or different the two maps are in guiding your journey.\n\n3. **Smooth**: Handles zero probabilities gracefully compared to KL Divergence - When one map suggests going down a street that the other doesn't (or vice versa), our navigator won't get flustered or lost. She'll find an alternative route, making sure you still reach your destination with minimal d\n\n## A picture is worth a thousand words\n\n::: {#cell-fig-one .cell layout-ncol='1' execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.optimize import linprog\n\ndef kl_divergence(p, q):\n    p = np.array(p)\n    q = np.array(q)\n    return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n\ndef js_divergence(p, q):\n    p = np.array(p)\n    q = np.array(q)\n    m = 0.5 * (p + q)\n    kl_mq = kl_divergence(m, q)\n    kl_mp = kl_divergence(m, p)\n    return 0.5 * (kl_mq + kl_mp)\n\n# Define two distributions\nx = np.linspace(-3, 3, 100)\np = np.exp(-x**2) / np.sqrt(2 * np.pi)  # Normal distribution, mean=0, std=1\nq = np.exp(-(x - 1)**2) / np.sqrt(2 * np.pi)  # Normal distribution, mean=1, std=1\n\n# Normalize distributions\np /= np.sum(p)\nq /= np.sum(q)\n\n# Compute divergences and their components\nkl_pq = kl_divergence(p, q)\nkl_qp = kl_divergence(q, p)\njs_pq = js_divergence(p, q)\nm = 0.5 * (p + q)\nkl_mq = kl_divergence(m, q)\nkl_mp = kl_divergence(m, p)\n\n# Visualization\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(12, 6))\n\n# Original distributions P and Q\nplt.plot(x, p, label=\"P: N(0,1)\", color=\"blue\")\nplt.plot(x, q, label=\"Q: N(1,1)\", color=\"red\")\n\n# KL(P || Q) component: P * log(P / Q)\nkl_pq_component = np.where(p != 0, p * np.log(p / q), 0)\nplt.plot(x, kl_pq_component, label=f\"KL(P || Q): {kl_pq:.4f}\", color=\"green\", linestyle=\"--\")\n\n# KL(Q || P) component: Q * log(Q / P)\nkl_qp_component = np.where(q != 0, q * np.log(q / p), 0)\nplt.plot(x, kl_qp_component, label=f\"KL(Q || P): {kl_qp:.4f}\", color=\"purple\", linestyle=\"--\")\n\n# JS(P || Q) components: 0.5 * (P * log(P / M) + Q * log(Q / M))\njs_pq_components = 0.5 * (np.where(p != 0, p * np.log(p / m), 0) +\n                         np.where(q != 0, q * np.log(q / m), 0))\nplt.plot(x, js_pq_components, label=f\"JS(P || Q): {js_pq:.4f}\", color=\"orange\", linestyle=\":\")\n\nplt.title(\"KL and JS Divergence Components Visualization\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Probability Density / Component\")\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![KL and JS Divergence Components Visualization](index_files/figure-html/fig-one-output-1.png){#fig-one width=982 height=529}\n:::\n:::\n\n\n::: {.callout-tip title=\"What is divergence components ?\" collapse=\"true\"}\nThe divergence components represent the point-wise contributions to the overall divergence measures between distributions P and Q.  \nThe area under each component curve equals the total divergence measure. Locations where components peak indicate regions where the distributions differ most substantially, providing insight into local distribution differences.\n:::\nThe plot visualizes the comparison between Kullback-Leibler (KL) and Jensen-Shannon (JS) divergences for two normal distributions $PN(0,1)$ and $QN(1,1)$.  \n\nThe blue and red curves show the original distributions, while the green and purple dashed lines represent the KL divergence components $KL(P||Q)$ and $KL(Q||P)$ respectively, demonstrating KL's asymmetry.     \n\nThe orange dotted line shows the JS divergence component, which is symmetric and combines information from both distributions through their midpoint M. \n\nThe legend includes numerical values for each divergence measure, with $KL(P||Q) ≠ KL(Q||P)$, illustrating KL's asymmetric nature, while JS maintains a single value representing the symmetric difference between the distributions.\n\n# A Brief Pause\n\nWhile KL and JS divergences focus on comparing the probabilistic structure of two distributions, they fail to capture spatial or geometric differences between distributions. This is especially important when comparing datasets where the \"location\" of the mass matters as much as the overall distribution.\n\nEnter the **Wasserstein Distance**, a metric that builds upon the idea of moving probability mass between distributions. By framing the comparison in terms of transportation costs, Wasserstein Distance offers a more comprehensive perspective. It measures not only the discrepancy in probabilities but also the effort required to align distributions spatially. This makes it particularly valuable in modern machine learning applications like Generative Adversarial Networks (GANs), domain adaptation, and shape analysis.\n\nIn the sections that follow, we will delve deeper into the Wasserstein Distance, exploring its mathematical foundation, intuitive interpretations, and practical applications.\n\n::: {.callout-note appearance=\"simple\"}\n\n## To be more clear\n\nWhile KL and JS divergences can be computed using simple pointwise operations (e.g., division, multiplication, and addition) on the probability density functions (PDFs), followed by integration or summation. In contrast, the Wasserstein Distance requires solving an optimization problem, typically using the Monge-Kantorovich mass transfer theorem or related techniques like the Sinkhorn algorithm (This will be our next story, so be ready). This makes it more computationally involved.\n:::\n\n# The Wasserstein distance \n\nWhen comparing distributions, the KL divergence helps us measure the expected logarithmic difference when using one distribution to approximate another - like measuring the accumulated inefficiency of following wrong directions. The Jensen-Shannon divergence builds on this by averaging two KL divergences, giving us a symmetric measure of dissimilarity between distributions.  \nAlthough KL and JS provide ways to compare distributions through their probability structures, they have limitations. \n\nLet's take a concrete example: imagine you're a data scientist who needs to compare customer purchasing patterns before and after a marketing campaign. In this case, you might want a measure that captures not only probability differences but also the actual 'distance' between values.  \nThis is where the Wasserstein distance comes in, with its elegant earth-moving metaphor: imagine your distributions as piles of earth, and you need to figure out the most efficient way to reshape one pile into the other.\n\nThis earth-moving perspective gives us a more intuitive way to measure distribution differences. Instead of just looking at probability ratios like KL divergence does, or symmetric probability differences like JS divergence does, Wasserstein distance considers how far probability mass needs to move to transform one distribution into another. This is particularly useful when dealing with distributions that don't overlap much, where KL and JS divergences might struggle to provide meaningful comparisons.\n\nSo while KL and JS work with probability ratios and log differences, Wasserstein explicitly incorporates the underlying geometry/distance of the space where the distributions live. \n\n## The Mathematical Journey\n\n### The Classical Approach: Monge's Vision\n\nGaspard Monge, an 18th-century French mathematician, first formulated this problem of optimal transportation who will later be express in terms of moving earth (hence the nickname \"Earth Mover's Distance\" or \"Monge-Kantorovich problem\").\nHis formulation is the Wasserstein distance for $p=1$:\n\n$$\nW_p(P, Q) = \\left( \\inf_{T} \\int_{\\mathbb{R}^d} \\|x - T(x)\\|^p \\, dP(x) \\right)^{1/p}\n$$\n\n$$W_p(μ, ν) = \\left(\\inf_{\\gamma \\in \\Gamma(\\mu, \\nu)} \\int_{X \\times Y} d(x,y)^p d\\gamma(x,y)\\right)^{1/p}$$ \n\n- A transport map $T(x)$ moves mass at point $x$ under distribution $P$ to some point under distribution $Q$. \n- The cost of moving mass from $x$ to $T(x)$ is given by $\\|x - T(x)\\|^p$, raised to the power $p$. \n- We seek the transport map $T$ that minimizes this total transport cost, denoted as $\\inf_T \\|x - T(x)\\|^p$.\n\n\nThink of this as finding the optimal way to move each grain of sand from one pile ($P$) to another ($Q$). The function $T(x)$ tells us where each grain should go, and we want to minimize the total effort of moving all the sand.\n\nBut there was a catch – sometimes this direct mapping approach proves impossible ($T(x)$ does not exist). Imagine trying to transform a single mountain ($P$) into two smaller hills ($Q$). How do you map one peak to two? This limitation led to a more flexible approach.\n\n::: {#cell-fig-KL .cell .column-page layout-ncol='1' execution_count=3}\n``` {.python .cell-code}\n# Set up the initial and target distributions of sand piles\nnp.random.seed(42)\npile_1 = np.random.normal(loc=0, scale=1, size=50)  # Smaller sample for clarity\npile_2 = np.random.normal(loc=2, scale=1.5, size=50)  # Target pile\n\n# Transport plan: simplest one-to-one matching (sorted values for illustration)\npile_1_sorted = np.sort(pile_1)\npile_2_sorted = np.sort(pile_2)\n\n# Create the multiplot\nfig, axes = plt.subplots(1, 3, figsize=(18, 5), sharey=False)\n\n# Plot initial pile\nsns.histplot(pile_1, kde=True, color=\"blue\", ax=axes[0])\naxes[0].set_title(\"Initial Pile of Sand\")\naxes[0].set_xlabel(\"Sand Position\")\naxes[0].set_ylabel(\"Frequency\")\n\n# Plot target pile\nsns.histplot(pile_2, kde=True, color=\"green\", ax=axes[1])\naxes[1].set_title(\"Target Pile of Sand\")\naxes[1].set_xlabel(\"Sand Position\")\n\n# Plot transport plan with aesthetic lines\nfor i in range(len(pile_1_sorted)):\n    axes[2].plot([0, 1], [pile_1_sorted[i], pile_2_sorted[i]], color=\"purple\", alpha=0.7, lw=1)\n\naxes[2].scatter([0]*len(pile_1_sorted), pile_1_sorted, color=\"blue\", label=\"Initial Positions\", zorder=5)\naxes[2].scatter([1]*len(pile_2_sorted), pile_2_sorted, color=\"green\", label=\"Target Positions\", zorder=5)\n\naxes[2].set_title(\"Transport Plan\")\naxes[2].set_xticks([0, 1])\naxes[2].set_xticklabels([\"Initial\", \"Target\"])\naxes[2].set_xlim(-0.5, 1.5)\naxes[2].set_xlabel(\"Sand Pile\")\naxes[2].set_ylabel(\"Sand Position\")\naxes[2].legend()\n\n# Adjust layout and display\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Visualization of the sandpile transport problem, including initial and target distributions as well as the transport plan.](index_files/figure-html/fig-kl-output-1.png){#fig-kl width=1712 height=464}\n:::\n:::\n\n\n### The Modern Solution: Kantorovich's Breakthrough\n\nLeonid Kantorovich revolutionized the field by allowing us to split and combine masses. His formulation looks like this:\n\n$$\nW_p(P, Q) = \\left( \\inf_{J \\in \\mathcal{J}(P, Q)} \\int_{\\mathbb{R}^d \\times \\mathbb{R}^d} \\|x - y\\|^p \\, dJ(x, y) \\right)^{1/p}\n$$\n\n- **J(x, y)**: A joint distribution over (x, y) pairs. Think of J as specifying how much \"mass\" at x under P is transported to y under Q.\n- **J(P, Q)**: The set of all possible joint distributions J whose marginals are P and Q.\n\n- Marginal Condition :  \n  - $\\int J(x, y) \\, dy = P(x)$, All mass at x under P is accounted for.  \n  - $\\int J(x, y) \\, dx = Q(y)$, All mass at y under Q is accounted for.\n\n- $∥x - y∥^p$ : The cost of transporting mass between x and y.\n  \nInstead of finding a direct map, we now look for a \"transport plan\" $J(x,y)$ that tells us how much mass to move from each point $x$ to each point $y$. It's like having a shipping manifest that details how much sand moves between any two locations.\n\nAn interesting point is the fact that the Kantorovich problems can be express a dual formula.\n\nIn this dual formulation, instead of finding a transport plan **J**, we find functions $\\phi(x)$ and $\\psi(y)$ that describe the \"potential energy\" of moving mass from **P** to **Q**. The supremum is taken over all measurable functions $\\psi$ and $\\phi$.\n\n$$\\mathcal{W}_p(P, Q) = \\sup_{\\psi, \\phi} \\left( \\int_{R^ d}\\psi(y)\\,dQ(y) - \\int_{R^d}\\phi(x)\\,dP(x) \\right)$$\n\nwhere $\\psi(y) - \\phi(x) \\leq \\|x - y\\|^p$\n\nSo, instead of finding a transport plan J, we find functions $\\phi(x)$ and $\\psi(y)$ that describe the \"potential energy\" of moving mass from distribution P to Q. These functions, often referred to as potentials or couplings, quantify the minimal cumulative cost required to transport mass from any point x in the source (P) to a point with zero potential energy in the target (Q), and vice versa. By optimizing over these potentials, we can efficiently compute the minimal cost of transporting all mass from P to Q.\n\nBut before seeing what does thies mean let's look some special case.\n\n## Special Cases and Practical Trick\n\n### The One-Dimensional Miracle\n\nIn one dimension, something magical happens. The Wasserstein distance becomes surprisingly simple to compute:\n\n$$\nW_p(P, Q) = \\left( \\int_{0}^1 \\left| F^{-1}(z) - G^{-1}(z) \\right|^p dz \\right)^{1/p}\n$$\n\nBecome for $p=2$ the simple :\nThis is like comparing two distributions by aligning their quantiles and measuring the average distance between corresponding points. It's particularly useful when working with simple univariate distributions.\n\n::: {#cell-fig-was .cell layout-ncol='1' execution_count=4}\n``` {.python .cell-code}\n# Generate data\nx = np.linspace(0, 8, 200)\ndist1 = stats.norm.pdf(x, loc=3, scale=0.5)\ndist2 = stats.norm.pdf(x, loc=4, scale=0.7)\n\n# Create plot\nplt.figure(figsize=(10, 6))\nplt.plot(x, dist1, label='Distribution P', color='#8884d8')\nplt.plot(x, dist2, label='Distribution Q', color='#82ca9d')\n\n# Add quantile lines\nquantile_points = [0.25, 0.5, 0.75]\nfor q in quantile_points:\n    q1 = stats.norm.ppf(q, loc=3, scale=0.5)\n    q2 = stats.norm.ppf(q, loc=4, scale=0.7)\n    plt.vlines(q1, 0, stats.norm.pdf(q1, loc=3, scale=0.5), \n               colors='#8884d8', linestyles='--', alpha=0.5)\n    plt.vlines(q2, 0, stats.norm.pdf(q2, loc=4, scale=0.7), \n               colors='#82ca9d', linestyles='--', alpha=0.5)\n    plt.plot([q1, q2], \n            [stats.norm.pdf(q1, loc=3, scale=0.5), \n             stats.norm.pdf(q2, loc=4, scale=0.7)], \n            'gray', alpha=0.3)\n\nplt.xlabel('x')\nplt.ylabel('Density')\nplt.title('Wasserstein Distance Visualization')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Visualization of the Wasserstein Distance between two normal distributions with annotated quantile lines.](index_files/figure-html/fig-was-output-1.png){#fig-was width=816 height=529}\n:::\n:::\n\n\n### The Gaussian Case: A Beautiful Formula\n\nWhen comparing two Gaussian distributions, we get an especially elegant result:\n\n$$\nW_2^2(P, Q) = \\|\\mu_1 - \\mu_2\\|^2 + \\text{trace}(\\Sigma_1 + \\Sigma_2 - 2(\\Sigma_1^{1/2} \\Sigma_2 \\Sigma_1^{1/2})^{1/2})\n$$\n\n- **Distance between the means:**\n  $\\|\\mu_1 - \\mu_2\\|_2$ Measures the Euclidean distance between the means of two distributions.\n- **Trace:**\n  $\\text{Tr}(\\Sigma_1 - \\Sigma_2)^2$ Measures differences in the covariance structures, where $\\Sigma_1$ and $\\Sigma_2$ are the covariance matrices of the respective distributions.\n\nThis formula neatly separates the difference between means from the difference in spread and shape.\n\n## Making It Practical: The Sinkhorn Algorithm\n\nIn real-world applications, when $P$ and $Q$ are discrete distribution,  computing the exact Wasserstein distance can be computationally intensive. Enter the Sinkhorn algorithm, which adds a touch of entropy regularization:\n\n$$\nW_p^\\lambda(P, Q) = \\inf_{J \\in \\mathcal{J}(P, Q)} \\int_{\\mathbb{R}^d \\times \\mathbb{R}^d} \\|x - y\\|^p dJ(x, y) + \\lambda \\int_{\\mathbb{R}^d \\times \\mathbb{R}^d} J(x, y) \\log J(x, y)\n$$\n\nThis regularization makes the computation much more tractable while still preserving the essential properties of the Wasserstein distance.\n\n::: {.callout-tip title=\"Mistral Nemo Explain 👨🏻‍🏫\" collapse=\"true\"}\n1. **Entropy Regularization**: The key idea behind entropy regularization is to add a term proportional to the negative entropy (or KL-divergence) of the transport plan **J** to the original optimization problem. This has two main effects:\n\t* It encourages **J** to be more \"uniform\" or \"entropic,\" making it easier to optimize.\n\t* It introduces a bias towards transports that are close to the uniform distribution, making the problem smoother and better suited for numerical methods like the Sinkhorn algorithm.\n2. **Sinkhorn Iterations**: The Sinkhorn algorithm is an iterative method that finds an approximate solution to the entropy-regularized Wasserstein distance minimization problem. It works by alternating between updating two potential functions, $\\phi(x)$ (for source distribution **P**) and $\\psi(y)$ (for target distribution **Q**), while keeping track of a transport plan **K** that is row-wise and column-wise stochastic (i.e., each row and column sums to 1). Here's how the algorithm progresses:\n\t* Initialize potential functions $\\phi^{(0)}(x) = \\log P(x)$ and $\\psi^{(0)}(y) = \\log Q(y)$, or some initial guess.\n\t* Iterate the following steps until convergence:\n\n\t\t1. **Row Update**: Compute a new transport plan **K** by normalizing the row-wise product of potentials:\n\t\t\t$$\n\t\t\tK_{ij} \\propto P_i Q_j e^{-\\|x_i - y_j\\|_p / \\lambda} e^{\\phi^{(t)}(x_i) + \\psi^{(t)}(y_j)}\n\t\t\t$$\n\t\t\twhere $P_i$ and $Q_j$ are the discrete probabilities for sources and targets, respectively.\n\n\t\t2. **Column Update**: Normalize the columns of **K** to obtain an updated transport plan **K'**, and then compute new potentials:\n\t\t\t$$\n\t\t\t\\phi^{(t+1)}(x_i) = \\log \\left( \\sum_j K'_{ij} e^{- \\|x_i - y_j\\|_p / \\lambda} e^{\\psi^{(t)}(y_j)} \\right)\n\t\t\t$$\n\t\t\t$$\n\t\t\t\\psi^{(t+1)}(y_j) = \\phi^{(t+1)}(x_0) + \\frac{\\|x_0 - y_j\\|_p}{\\lambda} - \\log Q_j\n\t\t\t$$\n\t\t\twhere $x_0$ is an arbitrary point (often chosen as the mean or median of **P**).\n\n3. **Convergence**: The Sinkhorn algorithm is guaranteed to converge for any initial potentials, and the limit transport plan **K** satisfies a fixed-point equation that corresponds to the solution of the entropy-regularized Wasserstein distance minimization problem. Moreover, as $\\lambda \\to 0$, the approximate solution converges to the exact solution of the original (unregularized) problem.\n\n4. **Advantages**: The main advantages of using the Sinkhorn algorithm with entropy regularization are:\n\t* It provides an efficient and easy-to-implement method for computing approximations of the Wasserstein distance between discrete distributions.\n\t* It can handle large-scale problems, as it only requires solving a sequence of linear systems at each iteration.\n\t* It is robust to initialization and converges quickly in practice.\n::: \n\n**To resume**:\nGiven two distributions **P** and **Q**, the core idea of the Wasserstein distance is to find the \"least-cost\" way to transport mass from **P** to **Q**. This problem can be formulated either as a Monge map (direct transportation) or a Kantorovich potential (joint distribution).  \nThe output is a scalar distance $\\mathcal{W}_p(\\textbf{P}, \\textbf{Q})$ that quantifies the minimal cumulative cost of transporting all mass from **P** to **Q**, where costs are measured using the $L_p$ norm. Optionally, an explicit transport plan or map can also be provided.  \n\nThis formulation enables us to compare and analyze distributions based on their relative transportation costs, making it a powerful tool in various applications such as machine learning, shape analysis, and optimal transport theory. In the next part, we will explore practical algorithms for computing the Wasserstein distance between discrete distributions.\n\n\n# Problem Solving : How to find the optimal path for 1D distribution ?\n\nNow let's focusing on explaining the optimal transport method for 1D distributions and its relation to Wasserstein distance. Yes, it is time for an an easy practical example.\n\n\n## Optimal Transport for 1D Distributions: Finding the Best Path  \n\nIn this section, we explore how to compute the **\"optimal transport plan\"** between two 1D distributions $p(x)$ and $q(x)$.  \nAs already explain, the concept of optimal transport lies at the heart of the Wasserstein distance, which measures the \"effort\" required to morph one distribution into another.  \n\nThis is particularly useful for comparing probability distributions, whether as a loss function in machine learning or for creating monitoring metrics.\n\nLet's explain what will happen in the code.\n\n## First Stage: The Cost Matrix  \n\nTo determine the optimal transport plan, we first define a **cost matrix** $C$. Each entry $C[i][j]$ represents the cost of moving \"mass\" from bin $i$ of $p(x)$ to bin $j$ of $q(x)$. For this example, we use the squared Euclidean distance between bins:\n\n$$\nC[i][j] = (i - j)^2\n$$\n\nThis choice ensures that movements over longer distances incur a higher cost, reflecting their physical or conceptual separation.\n\n## Second Stage : Linear Programming Approach for finding the best Transport Plan.\n\nThe optimal transport problem can be formulated as a linear programming (LP) problem. Our goal is to find a transport plan $T$, where $T[i][j]$ specifies the amount of mass to move from bin $i$ of $p(x)$ to bin $j$ of $q(x)$, such that:\n\n1. The **row sums** of $T$ equal $p$, ensuring all mass in $p(x)$ is transported:\n   $$\n   \\sum_{j} T[i][j] = p[i] \\quad \\forall i\n   $$\n\n2. The **column sums** of $T$ equal $q$, ensuring all mass in $q(x)$ is received:\n   $$\n   \\sum_{i} T[i][j] = q[j] \\quad \\forall j\n   $$\n\n3. The total cost is minimized:\n   $$\n   \\text{Minimize } \\sum_{i, j} T[i][j] \\cdot C[i][j]\n   $$\n\nWe use the `scipy.optimize.linprog` function to solve this LP problem. The result is a transport plan $T$ that minimizes the total cost while satisfying the marginal constraints.\n\n::: {.callout-tip appearance=\"simple\"}\n## Personal introspection\nAs I write this, I'm thinking back to some of my courses on path-finding algorithms like A* and Dijkstra. Maybe there's a link?\n:::\n## Third : Code Walkthrough\n\nLet's resume what we want to observe:\n\n1. **The Density Functions**  \n   The kernel density estimates (KDEs) of $p(x)$ and $q(x)$ give an intuitive view of the distributions. \n\n2. **The Cost Matrix**  \n   The heatmap of the cost matrix $C$ illustrates the squared distances between bins. Higher values indicate a higher cost of transportation.\n\n3. **The Optimal Transport Plan**  \n   By visualized $T$ as a heatmap we will shows the amounts of mass transported from each bin of $p(x)$ to $q(x)$.\n\nBelow are the results of applying the above concepts:\n\n::: {#92872435 .cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import linprog\n\nnp.random.seed(42)\n\nnb_b = 50\n\np = np.random.rand(nb_b)\np = p / p.sum()  \n\nq = np.random.rand(nb_b)\nq = q / q.sum()  \n\n# Create the cost matrix C based on squared Euclidean distance\nbins = len(p)  \nC = np.square(np.subtract.outer(np.arange(bins), np.arange(bins)))  # Squared distance\n\n# Flatten distributions and cost matrix for linear programming\nc = C.flatten()  # Cost vector (flattened cost matrix)\nA_eq = np.zeros((2 * bins, bins * bins))  # Equality constraints matrix\nb_eq = np.concatenate([p, q])  # Marginal sums for p and q\n\n# Build the A_eq matrix to enforce the marginals for p and q\nfor i in range(bins):\n    A_eq[i, i * bins: (i + 1) * bins] = 1  # For row sums to p[i]\n    A_eq[i + bins, i::bins] = 1  # For column sums to q[i]\n\n# Solve the linear programming problem\nresult = linprog(c, A_eq=A_eq, b_eq=b_eq, method='highs')\n\n# Retrieve the transport plan matrix T from the solution\nT = result.x.reshape((bins, bins))\n\n\ndef plot_density(p, q):\n    _, (ax1, ax2) = plt.subplots(2, 1, figsize=(5, 5))\n    \n    sns.kdeplot(p, color='blue', ax=ax1, linewidth=2, bw_adjust=0.3)\n    ax1.set_title('p(x)', fontsize=14, fontweight='bold')\n    ax1.set_xlabel('Bins', fontsize=14)\n    ax1.set_ylabel('Density', fontsize=14)\n\n    sns.kdeplot(q, color='orange', ax=ax2, linewidth=2, bw_adjust=0.3)\n    ax2.set_title('q(x)', fontsize=14, fontweight='bold')\n    ax2.set_xlabel('Bins', fontsize=14)\n    ax2.set_ylabel('Density', fontsize=14)\n\n    plt.tight_layout()\n    plt.show()\n\n\ndef plot_cost_matrix(C, bins):\n    plt.figure(figsize=(5,5))\n    sns.heatmap(C, annot=False, fmt='.1f', cmap=sns.color_palette(\"light:blue\", as_cmap=True), cbar=True,\n                xticklabels=[f'{i+1}' for i in range(bins)],\n                yticklabels=[f'{i+1}' for i in range(bins)],\n                cbar_kws={'label': 'Squared Distance'})\n    plt.title('Cost Matrix: Squared Euclidean Distance', fontsize=14, fontweight='bold')\n    plt.xlabel('Bins of Distribution q', fontsize=14)\n    plt.ylabel('Bins of Distribution p', fontsize=14)\n    plt.tight_layout()\n    plt.show()\n\n\ndef plot_transport_plan(T, bins):\n    plt.figure(figsize=(5,5))\n    sns.heatmap(T, annot=False, fmt='.2f', cmap=sns.color_palette(\"light:blue\", as_cmap=True), cbar=True,\n                xticklabels=[f'{i+1}' for i in range(bins)],\n                yticklabels=[f'{i+1}' for i in range(bins)],\n                cbar_kws={'label': 'Transport Amount'})\n    plt.title(\"Optimal Transport Plan\", fontsize=14, fontweight='bold')\n    plt.xlabel(\"Bins of Distribution q\", fontsize=14)\n    plt.ylabel(\"Bins of Distribution p\", fontsize=14)\n    plt.tight_layout()\n    plt.show()\n```\n:::\n\n\n::: {#cell-fig-cost .cell .column-page layout-ncol='2' execution_count=6}\n``` {.python .cell-code}\nplot_density(p, q)\n\nplot_cost_matrix(C, bins)\n```\n\n::: {.cell-output .cell-output-display}\n![Density plots of the two distributions p and q](index_files/figure-html/fig-cost-output-1.png){#fig-cost width=463 height=463}\n:::\n\n::: {.cell-output .cell-output-display}\n![Cost matrix based on squared Euclidean distance between bins of distributions p and q.](index_files/figure-html/fig-cost-output-2.png){#fig-cost width=465 height=463}\n:::\n:::\n\n\n::: {#cell-fig-transport .cell .column-page layout-ncol='2' execution_count=7}\n``` {.python .cell-code}\nplot_density(p, q)\n\nplot_transport_plan(T, bins)\n```\n\n::: {.cell-output .cell-output-display}\n![Density plots of the two distributions p and q](index_files/figure-html/fig-transport-output-1.png){#fig-transport width=463 height=463}\n:::\n\n::: {.cell-output .cell-output-display}\n![Optimal transport plan showing the transport amounts between bins of p and q.](index_files/figure-html/fig-transport-output-2.png){#fig-transport width=465 height=463}\n:::\n:::\n\n\nThe visualization of the optimal transport plan clearly shows how $p(x)$ is \"redistributed\" to align with $q(x)$.   \nEach nonzero element of `T` corresponds to a connection between $p[i]$ and $q[j]$, with the intensity reflecting the amount of mass transported.\n\nBut what's the connection with the Wasserstein Distance ?  \n\nHer, the optimal transport plan $T$ allows us to compute the **Wasserstein distance** as the square root of the total transport cost:\n\n$$\nW_2(p, q) = \\sqrt{\\sum_{i, j} T[i][j] \\cdot C[i][j]}\n$$\n\nThis metric provides a robust way to compare distributions, considering both the location and \"effort\" to match their mass. It is particularly advantageous over other metrics like Kullback-Leibler divergence for distributions with non-overlapping supports.\n\nOf course, it is a simple and non optimal exemple, and algorythm like Sinkhorn algorithm will work better.\n\n# This is the end\n\nUnderstanding distances between distributions is a fundamental challenge in data science. By framing the problem in terms of optimal transport – moving piles of sand from one distribution to another – it provides an intuitive and mathematically robust way to compare distributions.\n\nAs machine learning continues to evolve, tools like the Wasserstein distance become increasingly important. Whether you're evaluating GANs, WVAE, performing domain adaptation, or monitoring model drift, understanding how to measure and interpret distances between distributions is crucial. While the mathematics may seem daunting at first, the underlying intuition of optimal transport – finding the most efficient way to move probability mass – provides a concrete framework for thinking about these abstract concepts.\n\nKeep in mind that while we've focused on some specific implementations and special cases, there's much more to explore. The field of optimal transport continues to develop, with new algorithms and applications emerging regularly.\n\nRemember: Sometimes the best way to understand how things are different is to figure out what it takes to make them the same – and that's exactly what the Wasserstein distance helps us quantify\n\nThanks for your time reading my post !\n\n\n# References\n1. [\"Optimal Transport on Discrete Domains\" by Justin Solomon](https://people.csail.mit.edu/jsolomon/assets/optimal_transport.pdf?utm_source=chatgpt.com): This paper explores optimal transport theory in the context of discrete domains, providing insights into computational methods and applications. \n2. [\"A Short Introduction to Optimal Transport and Wasserstein Distance\" by Alex H. Williams](https://alexhwilliams.info/itsneuronalblog/2020/10/09/optimal-transport/?utm_source=chatgpt.com): This blog post offers an intuitive overview of optimal transport theory and Wasserstein distance.\n3. [\"Optimal Transport and Wasserstein Distance\" by Larry Wasserman](https://www.stat.cmu.edu/~larry/%3Dsml/Opt.pdf?utm_source=chatgpt.com): This PDF document delves into the mathematical foundations of optimal transport and Wasserstein distance, exploring their applications in statistics and machine learning. \n4. [\"Information Geometry Connecting Wasserstein Distance and Kullback-Leibler Divergence via the Entropy-Relaxed Transportation Problem\" by Shun-ichi Amari et al.](https://arxiv.org/abs/1709.10219?utm_source=chatgpt.com): This research paper presents a unified framework linking Wasserstein distance and KL divergence through entropy-relaxed optimal transport ; Deeper Math.\n5. [\"Mapping Between Two Gaussians Using Optimal Transport and the KL Divergence\" by Calvin McCarter](https://calvinmccarter.wordpress.com/2022/03/29/mapping-between-two-gaussians-using-optimal-transport-and-the-kl-divergence/?utm_source=chatgpt.com): This post explores the relationship between optimal transport and KL divergence\n6. [\"From GAN to WGAN\" by Lilian Weng](\"https://lilianweng.github.io/posts/2017-08-20-gan/\"): This comprehensive post delves into Generative Adversarial Networks (GANs) and introduces the concept of Wasserstein GANs (WGANs).\n7. [Shannon Entropy and Kullback-Leibler Divergence](https://www.stat.cmu.edu/~cshalizi/754/2006/notes/lecture-28.pdf)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}